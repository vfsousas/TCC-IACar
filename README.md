
  

# TCC-IACar

  
  
  

Este repositÃ³rio serÃ¡ utilizado pra o TCC da Faculdade Facens

  

**Aluno:** Vanderson Ferreira de Sousa

  

**RA:** 183140

  

## IntroduÃ§Ã£o

Recentemente, foi descoberto um tipo muito novo de IA que vocÃª pode construir a partir zero e sem a necessidade de qualquer biblioteca de Machine Learning ou DeepLearning. Esta IA revolucionÃ¡ria Ã© chamada ***Augmented Random Search(ARS)***.

Em uma implementaÃ§Ã£o muito simples, Ã© capaz de fazer exatamente o mesmo coisa que o Google Deep Mind fez em sua realizaÃ§Ã£o no ano passado que Ã© treinar uma IA para andar e correr em um campo.

No entanto, o ARS Ã© 100 vezes mais rÃ¡pido e 100 vezes mais poderosos alÃ©m de nÃ£o haver necessidade de algoritmos e estruturas sofisticadas

  

Todos os processos e uso dessa ARS foram retiradas do Paper ***Simple random search provides a competitive approach to reinforcement learning*** no qual pode ser acessado por esse [link](https://arxiv.org/abs/1803.07055) e o paper em PDF pode ser baixado nesse [link](https://arxiv.org/pdf/1803.07055.pdf)

A ARS foi desenvolvido por Horia Mania, Aurelia Guy e Benjamin Recht do ***Department of Electrical Engineering and Computer Science
University of California, Berkeley*** em 2018 e a intenÃ§Ã£o desse repositÃ³rio Ã© demostrar o uso desse algoritmo em um carro real construido por mim utilizando um Raspiberry Pi3 + hardware necessÃ¡rio para mover as rodas e 1 sensor de distancia que farÃ¡ leituras do ambiente e servirÃ¡ de input para o algoritmo do ARS.

  

VersÃ£o 1.0 do Carro
<img  src="imagens/carrov1.jpeg"  width=700  height=400>

  

Esse algoritmo estÃ¡ dentro da Ã¡rea de **Aprendizagem por ReforÃ§o**, que Ã© um tipo de aprendizagem usado em sistemas multi-agente no qual os agentes devem interagir no ambiente e aprenderem por conta prÃ³pria, ganhando recompensas positivas quando executam aÃ§Ãµes corretas e recompensas negativas quando executam aÃ§Ãµes que nÃ£o levem para o objetivo. A inteligÃªncia artificial aprende sem nenhum conhecimento prÃ©vio, adaptando-se ao ambiente e encontrando as soluÃ§Ãµes

  

### Basic Random Search (BRS)

O ARS Ã© uma inovaÃ§Ã£o do cÃ³digo BRS tendo sua ideia na Pesquisa AleatÃ³ria BÃ¡sica que Ã© escolher uma polÃ­tica parametrizada ğœ‹ğœƒ, chocar (ou perturbar) os parÃ¢metros ğœƒ aplicando + ğ›ğœ¹ e -ğ›ğœ¹ (onde ğ› <1 Ã© um ruÃ­do constante e ğœ¹ Ã© um nÃºmero aleatÃ³rio gerado a partir de uma distribuiÃ§Ã£o normal) .
Em seguida, aplica-se as aÃ§Ãµes baseadas em ğœ‹ (ğœƒ + ğ›ğœ¹) e ğœ‹ (ğœƒ-ğ›ğœ¹) e sÃ£o as recompensas r (ğœƒ + ğ›ğœ¹) e r (ğœƒ-ğ›ğœ¹) resultantes dessas aÃ§Ãµes.
Agora que temos as recompensas do ğœƒ perturbado, calcula-se a mÃ©dia Î” = 1 / N * Î£ [r (ğœƒ + ğ›ğœ¹) - r (ğœƒ-ğ›ğœ¹)] ğœ¹ para todos os ğœ¹ e atualizamos os parÃ¢metros ğœƒ usando Î” e uma taxa de aprendizagem ğ°.

ğœƒÊ²âºÂ¹ = ğœƒÊ² + ğ°.Î”

  

### Augmented Random Search (ARS)

O ARS Ã© uma versÃ£o aprimorada do BRS e contÃ©m trÃªs eixos de aprimoramentos que o tornam mais eficiente.

#### Dividindo pelo Desvio PadrÃ£o ğ¼áµ£

Conforme as iteraÃ§Ãµes continuam, a diferenÃ§a entre r (ğœƒ + ğ›ğœ¹) e r (ğœƒ-ğ›ğœ¹) pode variar significativamente, com a taxa de aprendizagem ğ° fixa, a atualizaÃ§Ã£o ğœƒÊ²âºÂ¹ = ğœƒÊ² + ğ°.Î” pode oscilar consideravelmente. Por exemplo, se ğ° = 0,01 e Î” = 10, entÃ£o ğ°.Î” serÃ¡ 0,1, mas se Î” se tornar 1000, ğ°.Î” se torna 10. Este tipo de variaÃ§Ã£o brutal prejudica a atualizaÃ§Ã£o. Lembre-se de que nosso objetivo Ã© fazer convergir ğœƒ para valores que maximizem recompensas.

Para evitar esse tipo de variaÃ§Ã£o, dividimos ğ°.Î” por ğ¼áµ£ (Desvio PadrÃ£o das recompensas coletadas).

 
### Normalizando os Estados

A normalizaÃ§Ã£o dos estados garante que as polÃ­ticas atribuam peso igual aos diferentes componentes dos estados. Por exemplo, suponha que um componente de estado assume valores na faixa [90, 100], enquanto outro componente de estado assume valores na faixa [-1, 1]. EntÃ£o, o primeiro componente de estado dominarÃ¡ a computaÃ§Ã£o, enquanto o segundo nÃ£o terÃ¡ nenhum efeito.

Para obter uma intuiÃ§Ã£o, considere uma mÃ©dia simples, suponha C1 = 91 e C2 = 1, a mÃ©dia serÃ¡ (C1 + C2) / 2 = 92/2 = 46. Agora suponha que C2 caiu drasticamente para o mÃ­nimo, C2 = - 1 A mÃ©dia serÃ¡ (91â€“1) / 2 = 45.

Observe que ele mal se moveu em relaÃ§Ã£o Ã  queda dramÃ¡tica de C2.
Agora vamos usar a normalizaÃ§Ã£o. Para C1 = 91, NC1 = (91-90) / (100-90) = 0,1,
para C2 = 1, NC2 = (1 - (-1)) / (1 - (- 1)) = 2/2 = 1.
A mÃ©dia normalizada serÃ¡ (0,1 + 1) / 2 = 0,55.
Agora, se C2 cai para -1, NC2 = (-1 - (- 1)) / 2 = 0 e a mÃ©dia normalizada torna-se (0,1 + 0) / 2 = 0,05.
Como vocÃª pode ver, a mÃ©dia foi muito afetada pela variaÃ§Ã£o acentuada de C2.

  

### Usando as instruÃ§Ãµes de melhor desempenho

Seria Ãºtil lembrar que nosso objetivo Ã© maximizar as recompensas coletadas. No entanto, estamos calculando a recompensa mÃ©dia em cada iteraÃ§Ã£o, o que significa que em cada iteraÃ§Ã£o calculamos 2N episÃ³dios, cada um seguindo ğœ‹ (ğœƒ + ğ›ğœ¹) e ğœ‹ (ğœƒ-ğ›ğœ¹), entÃ£o calculamos a mÃ©dia das recompensas coletadas r (ğœƒ + ğ›ğœ¹) e r (ğœƒ-ğ›ğœ¹) para todos os episÃ³dios 2N.

Isso apresenta algumas armadilhas porque se algumas das recompensas forem pequenas em comparaÃ§Ã£o com as outras, elas empurrarÃ£o a mÃ©dia para baixo.

Uma forma de solucionar esse problema Ã© classificar as recompensas em ordem decrescente com base na chave max (r (ğœƒ + ğ›ğœ¹), r (ğœƒ-ğ›ğœ¹)). Em seguida, use apenas as recompensas b principais (e suas respectivas perturbaÃ§Ãµes ğœ¹) no cÃ¡lculo da recompensa mÃ©dia.

Observe que quando b = N, o algoritmo serÃ¡ o mesmo sem este aprimoramento.

## Objetivo

O objetivo desse trabalho Ã© implementar, com seus devidos ajustes ao cenÃ¡rio do carro autÃ´nomo usando Raspiberry Pi3, um cÃ³digo completo de ARS totalmente baseado no Papers criado por 
Horia Mania e Aurelia Guy na Universidade de Berkley.
O trecho do cÃ³digo demonstrado no Paper Ã© o seguinte:
<img src='imagens/arscode.png' width=700  height=400>


## Resultados
Ainda a serem coletados