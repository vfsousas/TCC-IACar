{
 "cells": [
  {
   "source": [
    "# TCC-IACar\n",
    "\n",
    "Este reposit√≥rio ser√° utilizado pra o TCC da Faculdade Facens\n",
    "\n",
    "  \n",
    "  \n",
    "**Aluno:** Vanderson Ferreira de Sousa (vandersom@gmail.com)\n",
    "\n",
    "**RA:** 183140\n",
    "\n",
    "**Curso:** Intelig√™ncia Artificial Aplicada - Machine Learning\n",
    "\n",
    "**Orientador:** Johannes Von Lochter \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "## Introdu√ß√£o\n",
    "\n",
    "Recentemente, foi descoberto um tipo muito novo de IA que voc√™ pode construir a partir zero e sem a necessidade de qualquer biblioteca de Machine Learning ou DeepLearning. Esta IA revolucion√°ria √© chamada ***Augmented Random Search(ARS)***.\n",
    "\n",
    "Em uma implementa√ß√£o muito simples, √© capaz de fazer exatamente o mesmo coisa que o Google Deep Mind fez em sua realiza√ß√£o no ano passado que √© treinar uma IA para andar e correr em um campo.\n",
    "\n",
    "No entanto, o ARS √© 100 vezes mais r√°pido e 100 vezes mais poderosos al√©m de n√£o haver necessidade de algoritmos e estruturas sofisticadas\n",
    "\n",
    "  \n",
    "\n",
    "Todos os processos e uso dessa ARS foram retiradas do Paper ***Simple random search provides a competitive approach to reinforcement learning*** no qual pode ser acessado por esse [link](https://arxiv.org/abs/1803.07055) e o paper em PDF pode ser baixado nesse [link](https://arxiv.org/pdf/1803.07055.pdf)\n",
    "\n",
    "A ARS foi desenvolvido por Horia Mania, Aurelia Guy e Benjamin Recht do ***Department of Electrical Engineering and Computer Science\n",
    "University of California, Berkeley*** em 2018 e a inten√ß√£o desse reposit√≥rio √© demostrar o uso desse algoritmo em um carro real construido por mim utilizando um Raspiberry Pi3 + hardware necess√°rio para mover as rodas e 1 sensor de distancia que far√° leituras do ambiente e servir√° de input para o algoritmo do ARS.\n",
    "\n",
    "  \n",
    "\n",
    "Vers√£o 1.0 do Carro\n",
    "<img  src=\"imagens/carrov1.jpeg\"  width=700  height=400>\n",
    "\n",
    "  \n",
    "\n",
    "Esse algoritmo est√° dentro da √°rea de **Aprendizagem por Refor√ßo**, que √© um tipo de aprendizagem usado em sistemas multi-agente no qual os agentes devem interagir no ambiente e aprenderem por conta pr√≥pria, ganhando recompensas positivas quando executam a√ß√µes corretas e recompensas negativas quando executam a√ß√µes que n√£o levem para o objetivo. A intelig√™ncia artificial aprende sem nenhum conhecimento pr√©vio, adaptando-se ao ambiente e encontrando as solu√ß√µes\n",
    "\n",
    "  \n",
    "\n",
    "### Basic Random Search (BRS)\n",
    "\n",
    "O ARS √© uma inova√ß√£o do c√≥digo BRS tendo sua ideia na Pesquisa Aleat√≥ria B√°sica que √© escolher uma pol√≠tica parametrizada ùúãùúÉ, chocar (ou perturbar) os par√¢metros ùúÉ aplicando + ùõéùúπ e -ùõéùúπ (onde ùõé <1 √© um ru√≠do constante e ùúπ √© um n√∫mero aleat√≥rio gerado a partir de uma distribui√ß√£o normal) .\n",
    "Em seguida, aplica-se as a√ß√µes baseadas em ùúã (ùúÉ + ùõéùúπ) e ùúã (ùúÉ-ùõéùúπ) e s√£o as recompensas r (ùúÉ + ùõéùúπ) e r (ùúÉ-ùõéùúπ) resultantes dessas a√ß√µes.\n",
    "Agora que temos as recompensas do ùúÉ perturbado, calcula-se a m√©dia Œî = 1 / N * Œ£ [r (ùúÉ + ùõéùúπ) - r (ùúÉ-ùõéùúπ)] ùúπ para todos os ùúπ e atualizamos os par√¢metros ùúÉ usando Œî e uma taxa de aprendizagem ùù∞.\n",
    "\n",
    "ùúÉ ≤‚Å∫¬π = ùúÉ ≤ + ùù∞.Œî\n",
    "\n",
    "  \n",
    "\n",
    "### Augmented Random Search (ARS)\n",
    "\n",
    "O ARS √© uma vers√£o aprimorada do BRS e cont√©m tr√™s eixos de aprimoramentos que o tornam mais eficiente.\n",
    "\n",
    "#### Dividindo pelo Desvio Padr√£o ùûº·µ£\n",
    "\n",
    "Conforme as itera√ß√µes continuam, a diferen√ßa entre r (ùúÉ + ùõéùúπ) e r (ùúÉ-ùõéùúπ) pode variar significativamente, com a taxa de aprendizagem ùù∞ fixa, a atualiza√ß√£o ùúÉ ≤‚Å∫¬π = ùúÉ ≤ + ùù∞.Œî pode oscilar consideravelmente. Por exemplo, se ùù∞ = 0,01 e Œî = 10, ent√£o ùù∞.Œî ser√° 0,1, mas se Œî se tornar 1000, ùù∞.Œî se torna 10. Este tipo de varia√ß√£o brutal prejudica a atualiza√ß√£o. Lembre-se de que nosso objetivo √© fazer convergir ùúÉ para valores que maximizem recompensas.\n",
    "\n",
    "Para evitar esse tipo de varia√ß√£o, dividimos ùù∞.Œî por ùûº·µ£ (Desvio Padr√£o das recompensas coletadas).\n",
    "\n",
    " \n",
    "### Normalizando os Estados\n",
    "\n",
    "A normaliza√ß√£o dos estados garante que as pol√≠ticas atribuam peso igual aos diferentes componentes dos estados. Por exemplo, suponha que um componente de estado assume valores na faixa [90, 100], enquanto outro componente de estado assume valores na faixa [-1, 1]. Ent√£o, o primeiro componente de estado dominar√° a computa√ß√£o, enquanto o segundo n√£o ter√° nenhum efeito.\n",
    "\n",
    "Para obter uma intui√ß√£o, considere uma m√©dia simples, suponha C1 = 91 e C2 = 1, a m√©dia ser√° (C1 + C2) / 2 = 92/2 = 46. Agora suponha que C2 caiu drasticamente para o m√≠nimo, C2 = - 1 A m√©dia ser√° (91‚Äì1) / 2 = 45.\n",
    "\n",
    "Observe que ele mal se moveu em rela√ß√£o √† queda dram√°tica de C2.\n",
    "Agora vamos usar a normaliza√ß√£o. Para C1 = 91, NC1 = (91-90) / (100-90) = 0,1,\n",
    "para C2 = 1, NC2 = (1 - (-1)) / (1 - (- 1)) = 2/2 = 1.\n",
    "A m√©dia normalizada ser√° (0,1 + 1) / 2 = 0,55.\n",
    "Agora, se C2 cai para -1, NC2 = (-1 - (- 1)) / 2 = 0 e a m√©dia normalizada torna-se (0,1 + 0) / 2 = 0,05.\n",
    "Como voc√™ pode ver, a m√©dia foi muito afetada pela varia√ß√£o acentuada de C2.\n",
    "\n",
    "  \n",
    "\n",
    "### Usando as instru√ß√µes de melhor desempenho\n",
    "\n",
    "Seria √∫til lembrar que nosso objetivo √© maximizar as recompensas coletadas. No entanto, estamos calculando a recompensa m√©dia em cada itera√ß√£o, o que significa que em cada itera√ß√£o calculamos 2N epis√≥dios, cada um seguindo ùúã (ùúÉ + ùõéùúπ) e ùúã (ùúÉ-ùõéùúπ), ent√£o calculamos a m√©dia das recompensas coletadas r (ùúÉ + ùõéùúπ) e r (ùúÉ-ùõéùúπ) para todos os epis√≥dios 2N.\n",
    "\n",
    "Isso apresenta algumas armadilhas porque se algumas das recompensas forem pequenas em compara√ß√£o com as outras, elas empurrar√£o a m√©dia para baixo.\n",
    "\n",
    "Uma forma de solucionar esse problema √© classificar as recompensas em ordem decrescente com base na chave max (r (ùúÉ + ùõéùúπ), r (ùúÉ-ùõéùúπ)). Em seguida, use apenas as recompensas b principais (e suas respectivas perturba√ß√µes ùúπ) no c√°lculo da recompensa m√©dia.\n",
    "\n",
    "Observe que quando b = N, o algoritmo ser√° o mesmo sem este aprimoramento.\n",
    "\n",
    "## Objetivo\n",
    "\n",
    "O objetivo desse trabalho √© implementar, com seus devidos ajustes ao cen√°rio do carro aut√¥nomo usando Raspiberry Pi3, um c√≥digo completo de ARS totalmente baseado no Papers criado por \n",
    "Horia Mania e Aurelia Guy na Universidade de Berkley.\n",
    "O trecho do c√≥digo demonstrado no Paper √© o seguinte:\n",
    "<img src='imagens/arscode.png' width=700  height=400>\n",
    "\n",
    "\n",
    "## Resultados\n",
    "Os primeiros resultados podem ser visto no video abaixo\n",
    "[![LINK](https://img.youtube.com/vi/w3pzScq0zEQ/0.jpg)](https://www.youtube.com/watch?v=w3pzScq0zEQ)\n",
    "\n",
    "## Atualiza√ß√£o do Chassi do carro\n",
    "A vers√£o anterior do carro se mostrou muito instavel nos testes, al√©m de ser muito lento o uso do motor de passo junto com o sensor de distancia.\n",
    "A nova vers√£o vem com um chassi de acrilico mais refor√ßado, um \"eixo de dire√ß√£o\" controlado por um servo motor e 3 sensores de dire√ß√£o VL53l0X fixos\n",
    "<img src='imagens/carrov2.jpeg' width=700  height=400>\n",
    "\n",
    "Ap√≥s a atualiza√ß√£o do chassi do carro e inclus√£o do eixo de dire√ß√£o, esse foi o resultado obtido com o treino na √©poca 01\n",
    "[![LINK](https://i.ytimg.com/vi/Ea_02p__5Iw/hqdefault.jpg)](https://www.youtube.com/watch?v=Ea_02p__5Iw)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!cat bibliotecas.txt | xargs -n 1 pip3 install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importa√ß√£o de bibliotecas\n",
    "import RPi.GPIO as GPIO\n",
    "import time, copy\n",
    "import board, busio, adafruit_vl53l0x\n",
    "import serial\n",
    "import numpy as np\n",
    "from numpy import save\n",
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(\"RADAR.py\"))\n",
    "from RADAR import *\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup das portas logicas do Raspibery PI\n",
    "class SetupGPIO:\n",
    "    def __init__(self):\n",
    "        self.objGPIO = GPIO\n",
    "        self.objGPIO.cleanup() # limpa todos os estados de todas as portasmotorCar.\n",
    "        self.objGPIO.setmode(GPIO.BCM) #Definindi uso dos numeros das portas por canais\n",
    "    \n",
    "    def get_gpio(self):\n",
    "        return self.objGPIO\n",
    "\n",
    "    def clean(self):\n",
    "        self.objGPIO.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MotorCarro:\n",
    "    def __init__(self, raspGPIO, servo):\n",
    "        #GPIO Rodas dianteiras\n",
    "        self.WLF = 21 #Roda Direita Frente\n",
    "        self.WLB = 20 #Roda Direita pra tr√°s\n",
    "        self.WRF = 16 #Roda Esquerda Frente\n",
    "        self.WRB = 12 #Roda Esquerda Frente\n",
    "        self.servo = servo\n",
    "\n",
    "        #GPIO Rodas dianteiras\n",
    "        self.raspGPIO = raspGPIO\n",
    "        self.raspGPIO.setup(self.WLF, self.raspGPIO.OUT)\n",
    "        self.raspGPIO.setup(self.WLB, self.raspGPIO.OUT)\n",
    "        self.raspGPIO.setup(self.WRF, self.raspGPIO.OUT)\n",
    "        self.raspGPIO.setup(self.WRB, self.raspGPIO.OUT)\n",
    "        self.stop()\n",
    "\n",
    "    def stop(self):\n",
    "        self.raspGPIO.output(self.WLF, self.raspGPIO.LOW)\n",
    "        self.raspGPIO.output(self.WLB, self.raspGPIO.LOW)\n",
    "        self.raspGPIO.output(self.WRF, self.raspGPIO.LOW)\n",
    "        self.raspGPIO.output(self.WRB, self.raspGPIO.LOW)\n",
    "\n",
    "\n",
    "    def forward(self):\n",
    "        self.servo.rotateMotor('center')\n",
    "        self.raspGPIO.output(self.WLF, self.raspGPIO.HIGH)\n",
    "        self.raspGPIO.output(self.WRF, self.raspGPIO.HIGH)\n",
    "\n",
    "    \n",
    "\n",
    "    def backward(self):\n",
    "        self.servo.rotateMotor('center')\n",
    "        self.raspGPIO.output(self.WLB, self.raspGPIO.HIGH)\n",
    "        self.raspGPIO.output(self.WRB, self.raspGPIO.HIGH)\n",
    "\n",
    "        \n",
    "    def left_forward(self):\n",
    "        self.servo.rotateMotor('left')\n",
    "        self.raspGPIO.output(self.WLF, self.raspGPIO.HIGH)\n",
    "        self.raspGPIO.output(self.WRF, self.raspGPIO.HIGH)\n",
    "\n",
    "\n",
    "\n",
    "    def right_forward(self):\n",
    "        self.servo.rotateMotor('right')\n",
    "        self.raspGPIO.output(self.WLF, self.raspGPIO.HIGH)\n",
    "        self.raspGPIO.output(self.WRF, self.raspGPIO.HIGH)\n",
    "\n",
    "    def left_backward(self):\n",
    "        self.servo.rotateMotor('left')\n",
    "        self.raspGPIO.output(self.WLB, self.raspGPIO.HIGH)\n",
    "        self.raspGPIO.output(self.WRB, self.raspGPIO.HIGH)\n",
    "\n",
    "    def right_backward(self):\n",
    "        self.servo.rotateMotor('right')\n",
    "        self.raspGPIO.output(self.WLB, self.raspGPIO.HIGH)\n",
    "        self.raspGPIO.output(self.WRB, self.raspGPIO.HIGH)\n",
    "    \n",
    "    def movimentacarro(self, movimento):\n",
    "        if movimento==0:\n",
    "            self.forward()\n",
    "        elif movimento==1:\n",
    "             self.backward()\n",
    "        elif movimento==2:\n",
    "             self.left_forward()\n",
    "        elif movimento==3:\n",
    "             self.right_forward()\n",
    "        elif movimento==4:\n",
    "             self.left_backward()\n",
    "        elif movimento==5:\n",
    "             self.right_backward()\n",
    "        time.sleep(0.5)\n",
    "\n",
    "        self.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ServoMotor:\n",
    "    def __init__(self, raspGPIO):\n",
    "        self.position = ''\n",
    "        self.servoPIN = 18\n",
    "        self.raspGPIO = raspGPIO\n",
    "        self.raspGPIO.setup(self.servoPIN, GPIO.OUT)\n",
    "        self.servo = self.raspGPIO.PWM(self.servoPIN, 50) # GPIO 18 for PWM with 50Hz\n",
    "        self.servo.start(0)\n",
    "        time.sleep(2)\n",
    "\n",
    "        duty = 11\n",
    "\n",
    "        while duty >= 7:\n",
    "            self.servo.ChangeDutyCycle(duty)\n",
    "            time.sleep(0.3)\n",
    "            self.servo.ChangeDutyCycle(0)\n",
    "            time.sleep(0.7)\n",
    "            duty -= 1\n",
    "    \n",
    "    def stop(self):\n",
    "        self.servo.stop()\n",
    "    \n",
    "    def rotateMotor(self, position):\n",
    "        print(position)\n",
    "        if 'center' in position and 'center' not in self.position :\n",
    "            print('position center')\n",
    "            self.servo.ChangeDutyCycle(9)\n",
    "            time.sleep(0.3)\n",
    "            self.servo.ChangeDutyCycle(0)\n",
    "            time.sleep(0.7)\n",
    "            self.position = 'center'\n",
    "\n",
    "\n",
    "        \n",
    "        if 'left' in position and 'left' not in self.position:\n",
    "            print('position left')\n",
    "            self.servo.ChangeDutyCycle(11)\n",
    "            time.sleep(0.3)\n",
    "            self.servo.ChangeDutyCycle(0)\n",
    "            time.sleep(0.7)            \n",
    "            self.position = 'left'\n",
    "\n",
    "\n",
    "        if 'right' in position and 'right' not in self.position:\n",
    "            print('position right')\n",
    "            self.servo.ChangeDutyCycle(7)\n",
    "            time.sleep(0.3)\n",
    "            self.servo.ChangeDutyCycle(0)\n",
    "            time.sleep(0.7)            \n",
    "            self.position = 'right'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir(base):\n",
    "    ''''\n",
    "    Cria diret√≥rios para salvar as matrizes \n",
    "    '''\n",
    "    path = os.path.join('save', base)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    return path\n",
    "\n",
    "def salvaMatrizes(matrizNome, epoch, matrizValor):\n",
    "    ''''\n",
    "    Cria diret√≥rios para salvar as matrizes \n",
    "    Salva as matrizes com o nome e o valor passado como parametro\n",
    "    '''\n",
    "    today=datetime.today().strftime('%Y-%m-%d')\n",
    "    mkdir(today)\n",
    "    daytime=datetime.today().strftime('%H:%M:%S')\n",
    "    np.savez(('save/%s/%s_%s_%s.npz' % (today, matrizNome, epoch, daytime)), matrizValor)\n",
    "\n",
    "def carregaMatriz(pasta, matrizNome):\n",
    "    ''''\n",
    "    Carrega as matrizes salvas no diret√≥rio\n",
    "    '''\n",
    "    path = os.path.join('save', pasta, matrizNome)\n",
    "    load = np.load(path)\n",
    "    return load['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CarEnv:\n",
    "    def __init__(self, motorCarro, radar):\n",
    "        self.actions_space = ['forward', 'backward', 'leftforward', 'rightforward', 'leftbackward', 'rightbackward']\n",
    "        self.initialradarpositions = radar.get_distancias()\n",
    "        self.observation_space = np.array(np.zeros([len(self.initialradarpositions), len(self.actions_space)]))\n",
    "        self.state = self.initialradarpositions\n",
    "        self.done = False\n",
    "        self.input_size = len(self.initialradarpositions)\n",
    "        self.output_size = len(self.actions_space)\n",
    "        self.motorCar =  motorCarro\n",
    "        self.finishCount = 0\n",
    "        self.radar = radar\n",
    "    \n",
    "    def finish(self, state):\n",
    "        '''\n",
    "        Quando o carro se movimentar 3 vezes para a frente sem parar √© o objetivo dele\n",
    "        '''\n",
    "        print(\"STATE FINISH:\", state)\n",
    "        if(state[1]>15  and state[3]>15):\n",
    "            self.finishCount+=1\n",
    "        else:\n",
    "            self.finishCount=0\n",
    "        \n",
    "        print(\"Count FINISH:\", self.finishCount)\n",
    "\n",
    "        if self.finishCount>=3:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def step(self, action):\n",
    "        self.take_action(action)\n",
    "        self.state = self.getState()\n",
    "        self.done = self.finish(self.state)\n",
    "        stepP = copy.deepcopy(self.getReward()), self.state, self.done\n",
    "        return stepP\n",
    "\n",
    "    def getState(self):\n",
    "        try:\n",
    "            self.state = self.radar.get_distancias()\n",
    "        finally:\n",
    "           self.motorCar.movimentacarro(0)\n",
    "        return self.state\n",
    "        \n",
    "    def take_action(self, action):\n",
    "        movPosition = np.where(action == np.max(action)) \n",
    "        l = list(action).index(np.max(action))\n",
    "        print('action', l, np.max(action), action)\n",
    "        self.motorCar.movimentacarro(l)\n",
    "\n",
    "    def getReward(self, action):\n",
    "        #f = lambda x: 10 if  x>100 else -10\n",
    "        #return f(max(self.state[:5]))\n",
    "        retVal = -1\n",
    "        if(self.state[0]>15  and self.state[1]>15  and self.state[2]>15  and self.state[3]>15):\n",
    "            retVal=1\n",
    "            if action == 0 or action == 2 or action == 3:\n",
    "                retVal=10\n",
    "\n",
    "        return retVal\n"
   ]
  },
  {
   "source": [
    "### Inicializa√ß√£o dos Hiperparametros\n",
    "Neste parte implementaremos a seguinte parte do c√≥digo\n",
    "\n",
    "<img src=\"imagens/ars_part1.png\" width=600 heigth=400>\n",
    "\n",
    "Inicialmente vamos usar steps_size=10 e epsodes=10 apenas para fins de compara√ß√£o de resultados, ja que o ambiente √© real de um carro autonomo\n",
    "\n",
    "self.directions = Total de matrizes de pertuba√ß√µes a serem contruidas ***number of directions sampled per iteration N***\n",
    "\n",
    "self.best_directions = Total de matrizes com recompensas melhores \n",
    "\n",
    "Nunca as matrizes de pertuba√ß√£o pode ser maior que as matrizes de recompensas, por isso o uso do assert\n",
    "\n",
    "***(number of top-performing directions to use b (b < N is allowed only for V1-t and V2-t)***\n",
    "\n",
    "self.noise = noise ŒΩ"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hiperparametros():\n",
    "    def __init__(self):\n",
    "        self.epochs = 1000\n",
    "        self.epsodes = 10\n",
    "        self.lr = 0.02\n",
    "        self.directions = 6\n",
    "        self.best_directions = 2\n",
    "        assert self.best_directions <= self.directions\n",
    "        self.noise = 0.03\n",
    "        self.seed = 1\n",
    "        self.env_nome = ''\n"
   ]
  },
  {
   "source": [
    "Os bons resultados desse algoritmo se d√£o fortemente por causa da normaliza√ß√£o dos dados\n",
    "\n",
    "De acordo com documento, a normaliza√ß√£o √© necess√°rio por:\n",
    "\n",
    "\"A normaliza√ß√£o de estados usada por V2 √© semelhante ao clareamento de dados usado em tarefas de regress√£o, e\n",
    "intuitivamente, garante que as pol√≠ticas atribuam peso igual aos diferentes componentes dos estados. Para\n",
    "obter intui√ß√£o de por que isso pode ajudar, suponha que uma coordenada de estado s√≥ tenha valores no intervalo\n",
    "90, 100 enquanto outro componente de estado assume valores na faixa -1, 1. Ent√£o, pequenas mudan√ßas em\n",
    "o ganho de controle em rela√ß√£o √† primeira coordenada de estado levaria a mudan√ßas maiores nas a√ß√µes\n",
    "ent√£o, o mesmo tamanho muda em rela√ß√£o ao segundo componente de estado. Portanto, o clareamento permite\n",
    "a explora√ß√£o isotr√≥pica de pesquisa aleat√≥ria para ter igual influ√™ncia sobre os v√°rios componentes de estado\"\n",
    "\n",
    "\n",
    "Nesse caso vamos normalizar os valores recebidos pelo radar que varia entre 1 e 899, que seriam equivalentes h√° 1cm e 89,9cm. \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizacao dos estados (Standardization)\n",
    "class Normalizacao():\n",
    "    def __init__(self, inputs):\n",
    "        '''\n",
    "        Inicializa todos os parametros utilizados durante a normaliza√ß√£o\n",
    "        Keyword arguments:\n",
    "        inputs -- array dos valores recebidos do sensor de distancia, ex: [30, 6, 3, 4, 8, 91, 819]\n",
    "        '''\n",
    "        self.n = np.zeros(inputs) #agregador de estados descobertos desde o inicio\n",
    "        self.mean = np.zeros(inputs) #m√©dia de todos os valores de input\n",
    "        self.mean_diff = np.zeros(inputs) #usado para o calculo da variancia\n",
    "        self.var = np.zeros(inputs) #guarda os valores de variancia\n",
    "    \n",
    "    def observe(self, inputs):\n",
    "        '''\n",
    "        Realiza o calculo da variancia nos dados recebidos do sensor de movimento\n",
    "        Keyword arguments:\n",
    "        inputs -- array dos valores recebidos do sensor de distancia, ex: [30, 6, 3, 4, 8, 91, 819]\n",
    "        '''\n",
    "        self.n +=1. #indica em que a√ß√£o √© a atual\n",
    "        last_mean = self.mean.copy() #guarda o valor da ultima m√©dia realizada \n",
    "        self.mean += (inputs - self.mean) / self.n #atualizando a m√©dia baseada na quantidade de a√ß√µes ja realizadas\n",
    "        self.mean_diff += (inputs - last_mean) * (inputs - self.mean) # pega a diferen√ßa atual e a m√©dia antiga\n",
    "        self.var = (self.mean_diff/self.n).clip(min = 1e-2) #realiza o calculo da variancia, e limita o valor minino em 0.01\n",
    "\n",
    "    def normalize(self, inputs):\n",
    "        ''''\n",
    "        Realiza o calculo da normaliza√ß√£o(Padroniza√ß√£o) dividindo realizando x- m√©dia(x) / desvio padr√£o de X\n",
    "        Assim deixando todos os valores dentro da escala -1 e 1\n",
    "        Essa forma √© mais robusta contra outliers\n",
    "\n",
    "        Keyword arguments:\n",
    "        inputs -- array dos valores recebidos do sensor de distancia, ex: [30, 6, 3, 4, 8, 91, 819]\n",
    "\n",
    "        Return:\n",
    "        Normalized - Valores normalizados entre -1 e 1\n",
    "        '''\n",
    "        obs_mean = self.mean #Qual a m√©dia atual\n",
    "        obs_std = np.sqrt(self.var) #Calcula Desvio padr√£o\n",
    "        normalized = (inputs - obs_mean) / obs_std #(valor a ser nomalizado - m√©dia) / desvio padr√£o \n",
    "        return normalized"
   ]
  },
  {
   "source": [
    "Neste parte implementaremos as seguintes partes do c√≥digo\n",
    "\n",
    "<img src=\"imagens/ars_part2.png\" width=600 heigth=400>\n",
    "\n",
    "<img src=\"imagens/ars_part3.png\" width=600 heigth=400>\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Politicas():\n",
    "    '''\n",
    "        A explora√ß√£o acontece no espa√ßo das politicas depois que todo um epis√≥dio e steps dele foram executados\n",
    "        Diferente de outras IAs que usam explora√ß√£o por ambientes e a√ß√µes executadas\n",
    "    '''\n",
    "\n",
    "    def __init__(self, input_size, output_size):\n",
    "        '''\n",
    "        Keyword arguments:\n",
    "        input_size -- Numeros de entradas\n",
    "        output_size -- Numeros de sa√≠das\n",
    "\n",
    "        Return:\n",
    "        '''\n",
    "        self.theta = np.zeros((output_size, input_size)) #Cria uma matrix de pesos inicializado em zeros\n",
    "        #Estamos seguindo o m√©todo do artivo pelo lado esquerdo, por isso usamos (output_size, input_size)\n",
    "    \n",
    "    def evaluate(self, input, delta=None, direction=None):\n",
    "        ''''\n",
    "        De acordo com a dire√ß√£o passada em direction √© atualizada a matriz de pesos\n",
    "\n",
    "        Keyword arguments:\n",
    "        inputs -- array dos valores recebidos do sensor de distancia, ex: [30, 6, 3, 4, 8, 91, 819]\n",
    "        delta -- Matrix de pertuba√ß√£o dos numeros \n",
    "        direction -- Indica a dire√ß√£o do calculo para positivo ou negativo\n",
    "        '''\n",
    "\n",
    "        if direction is None:\n",
    "            return self.theta.dot(input) #retorna a matriz de pesos que multiplica com as entradas, sem pertuba√ß√µes\n",
    "        elif direction == 'positive':\n",
    "            return (self.theta + hp.noise * delta).dot(input) #retorna a matriz de pesos que multiplica com as entradas, com ruido de explora√ß√£o mais pertuba√ß√µes positivas\n",
    "        else: \n",
    "            return (self.theta - hp.noise * delta).dot(input)#retorna a matriz de pesos que multiplica com as entradas, com ruido de explora√ß√£o mais pertuba√ß√µes negativas\n",
    "    \n",
    "    def samples_deltas(self):\n",
    "        '''\n",
    "        Gerando matrix de pertuba√ß√£o, matrix com numeros aleat√≥rios\n",
    "        '''\n",
    "        return [np.random.randn(*self.theta.shape) for _ in range(hp.directions)]\n",
    "        #retorna uma matrix com distribui√ß√£o normal para o todas as matrizes de pertuba√ßao \n",
    "    \n",
    "    def update(self, rollouts, sigma_r):\n",
    "        '''\n",
    "        Item 7 do Algoritmo, fazendo a atualiza√ß√£o dos pesos\n",
    "\n",
    "        Keyword arguments:\n",
    "        rollouts -- conjunto de recompensa positiva, conjunto recompensa negativa e a matrizx de numeros aleat√≥rios\n",
    "        sigma_r -- Indica o desvio padr√£o da recompensa\n",
    "        '''\n",
    "        step = np.zeros(self.theta.shape) #Inicializa com as dimens√µes de pesos\n",
    "        for r_pos, r_neg, d in rollouts:\n",
    "            step += (r_pos - r_neg) * d #Somatoria das recompensas positivas e negativas e a multicao do delta\n",
    "        self.theta += hp.lr / (hp.best_directions * sigma_r) * step #Atualizando a matriz  de pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore(env, normalizer, policy, direction = None, delta=None):\n",
    "    '''\n",
    "         Faz a explora√ß√£o do ambiente enquanto n√£o finalizar e n√£o terminar as execu√ß√µes do epsodio\n",
    "      \n",
    "    '''\n",
    "    state = env.getState() #ler o ambiente radar\n",
    "    done = False #inicia em False o objetivo\n",
    "    num_plays = 0. #contador de rodadas no epis√≥dio\n",
    "    sum_rewards = 0 #soma das recompensas\n",
    "    while not done or num_plays < hp.epsodes:\n",
    "       normalizer.observe(state) #Atualiza o calculo da variancia nos dados recebidos do sensor de movimento\n",
    "       state = normalizer.normalize(state) #Realiza o calculo da normaliza√ß√£o(Padroniza√ß√£o) deixando todos os estados entre -1 e 1\n",
    "       action = policy.evaluate(state, delta, direction) #atualizada a matriz de pesos de acordo com a dire√ßao selecionada e retirna \n",
    "       reward, state, done = env.step(action) #Executa a a√ß√£o selecionada e retirna a nova leitura do ambiente e se foi finalizado \n",
    "       #reward = max(min(reward, 1), -1)#evita outlier nas recompensas\n",
    "       print('Execucao: ', num_plays, ', State: ', state, ', Action: ', action,', Recompensa: ', reward, 'Finalizado: ', done)\n",
    "       sum_rewards += reward #Soma das recompensas\n",
    "       num_plays +=1 #atualida o numero da rodada\n",
    "    return sum_rewards\n",
    "\n"
   ]
  },
  {
   "source": [
    "Neste parte implementaremos as seguintes partes do c√≥digo\n",
    "\n",
    "<img src=\"imagens/ars_part4.png\" width=600 heigth=400>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Treinando\n",
    "def train(env, policy, normalizer, hp):\n",
    "    ''''\n",
    "      Realiza o treinamento da rede\n",
    "   \n",
    "    '''\n",
    "    loadMatrixPositiveFilename = 'positive_rewards_6_16:57:05.npz'\n",
    "    LoadMatrixFolder = '2020-12-14'\n",
    "    loadMatrixNegativeFilename = 'negative_rewards_6_16:57:05.npz'\n",
    "    DeltaFilename = 'deltas_6_16:57:05.npz'\n",
    "    for epoch in range(hp.epochs):\n",
    "        \n",
    "        if loadMatrixPositiveFilename and loadMatrixNegativeFilename and LoadMatrixFolder and DeltaFilename:\n",
    "            deltas = carregaMatriz(LoadMatrixFolder, loadMatrixPositiveFilename) #Inicializacao das pertubacoes (deltas) e as recompensas negativas e positivas)\n",
    "            #positive_rewards = \n",
    "            negative_rewards = carregaMatriz(LoadMatrixFolder, DeltaFilename)\n",
    "        else:\n",
    "            deltas = policy.samples_deltas() #Inicializacao das pertubacoes (deltas) e as recompensas negativas e positivas)\n",
    "            positive_rewards = [0] * hp.directions #inicializando a matriz de recompensas positivas\n",
    "            negative_rewards = [0] * hp.directions #inicializando a matriz de recompensas negativas\n",
    "\n",
    "        #obtendo as recompensas na direcao positiva\n",
    "        for k in range(hp.directions):\n",
    "            positive_rewards[k] = explore(env, normalizer, policy, direction='positive', delta=deltas[k])\n",
    "            \n",
    "        #obtendo recompensa na direcao negativa\n",
    "        for k in range(hp.directions):\n",
    "            positive_rewards[k] = explore(env, normalizer, policy,  direction='negative', delta=deltas[k])\n",
    "        \n",
    "        #obtendo todas as recompensas positivas e negativas para computar o desvio dessas recompensas\n",
    "        all_reward = np.array(positive_rewards + negative_rewards)\n",
    "        sigma_r = all_reward.std()\n",
    "\n",
    "        #ordenacao dos rollouts e selecao das melhores direcoes\n",
    "        scores = {k: max(r_pos, r_neg) for k, (r_pos, r_neg) in enumerate(zip(positive_rewards, negative_rewards))}\n",
    "        order = sorted(scores.keys(), key=lambda x: scores[x], reverse=True)[:hp.best_directions]\n",
    "        rollouts = [(positive_rewards[k], negative_rewards[k], deltas [k]) for k in order]\n",
    "\n",
    "        #atualizacao de politica\n",
    "        policy.update(rollouts, sigma_r)\n",
    "\n",
    "        #impressao da recompensa\n",
    "        reward_evaluation = explore(env, normalizer, policy)\n",
    "        print('Step', step, 'Reward:',reward_evaluation )\n",
    "\n",
    "        salvaMatrizes('positive_rewards', epoch, positive_rewards)\n",
    "        salvaMatrizes('negative_rewards', epoch, negative_rewards)\n",
    "    s   alvaMatrizes('deltas', epoch, deltas)\n",
    "\n",
    "    \n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[0]\n"
    }
   ],
   "source": [
    "\n",
    "raspGPIO  = SetupGPIO()\n",
    "raspGPIO = raspGPIO.get_gpio()\n",
    "servo = ServoMotor(raspGPIO)\n",
    "motorCar = MotorCarro(raspGPIO, servo)\n",
    "radar = radar_new(raspGPIO, motorCar, servo)\n",
    "radar.initialize()\n",
    "print(radar.get_distancias())\n",
    "carEnv = CarEnv(motorCar, radar)\n",
    "hp = Hiperparametros()\n",
    "np.random.seed(hp.seed)\n",
    "police = Politicas(carEnv.input_size, carEnv.output_size)\n",
    "normalizer = Normalizacao(carEnv.input_size)\n",
    "train(carEnv,police,normalizer,hp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "tcc",
   "display_name": "tcc",
   "metadata": {
    "interpreter": {
     "hash": "85595fc6e4c66cea5298aa77223b4d67754825b590a0f7ffb3f639c26cc9de1a"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}