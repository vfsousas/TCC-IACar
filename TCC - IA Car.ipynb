{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!cat bibliotecas.txt | xargs -n 1 pip3 install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'RPi'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-43c07c2b83f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Importação de bibliotecas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mRPi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGPIO\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mGPIO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mboard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbusio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madafruit_vl53l0x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mserial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'RPi'"
     ]
    }
   ],
   "source": [
    "#Importação de bibliotecas\n",
    "import RPi.GPIO as GPIO\n",
    "import time, copy\n",
    "import board, busio, adafruit_vl53l0x\n",
    "import serial\n",
    "import numpy as np\n",
    "from numpy import save\n",
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(\"RADAR.py\"))\n",
    "from RADAR import *\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup das portas logicas do Raspibery PI\n",
    "class SetupGPIO:\n",
    "    def __init__(self):\n",
    "        self.objGPIO = GPIO\n",
    "        self.objGPIO.cleanup() # limpa todos os estados de todas as portasmotorCar.\n",
    "        self.objGPIO.setmode(GPIO.BCM) #Definindi uso dos numeros das portas por canais\n",
    "    \n",
    "    def get_gpio(self):\n",
    "        return self.objGPIO\n",
    "\n",
    "    def clean(self):\n",
    "        self.objGPIO.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MotorCarro:\n",
    "    def __init__(self, raspGPIO, servo):\n",
    "        #GPIO Rodas dianteiras\n",
    "        self.WLF = 21 #Roda Direita Frente\n",
    "        self.WLB = 20 #Roda Direita pra trás\n",
    "        self.WRF = 16 #Roda Esquerda Frente\n",
    "        self.WRB = 12 #Roda Esquerda Frente\n",
    "        self.servo = servo\n",
    "\n",
    "        #GPIO Rodas dianteiras\n",
    "        self.raspGPIO = raspGPIO\n",
    "        self.raspGPIO.setup(self.WLF, self.raspGPIO.OUT)\n",
    "        self.raspGPIO.setup(self.WLB, self.raspGPIO.OUT)\n",
    "        self.raspGPIO.setup(self.WRF, self.raspGPIO.OUT)\n",
    "        self.raspGPIO.setup(self.WRB, self.raspGPIO.OUT)\n",
    "        self.stop()\n",
    "\n",
    "    def stop(self):\n",
    "        self.raspGPIO.output(self.WLF, self.raspGPIO.LOW)\n",
    "        self.raspGPIO.output(self.WLB, self.raspGPIO.LOW)\n",
    "        self.raspGPIO.output(self.WRF, self.raspGPIO.LOW)\n",
    "        self.raspGPIO.output(self.WRB, self.raspGPIO.LOW)\n",
    "\n",
    "\n",
    "    def forward(self):\n",
    "        self.servo.rotateMotor('center')\n",
    "        self.raspGPIO.output(self.WLF, self.raspGPIO.HIGH)\n",
    "        self.raspGPIO.output(self.WRF, self.raspGPIO.HIGH)\n",
    "\n",
    "    \n",
    "\n",
    "    def backward(self):\n",
    "        self.servo.rotateMotor('center')\n",
    "        self.raspGPIO.output(self.WLB, self.raspGPIO.HIGH)\n",
    "        self.raspGPIO.output(self.WRB, self.raspGPIO.HIGH)\n",
    "\n",
    "        \n",
    "    def left_forward(self):\n",
    "        self.servo.rotateMotor('left')\n",
    "        self.raspGPIO.output(self.WLF, self.raspGPIO.HIGH)\n",
    "        self.raspGPIO.output(self.WRF, self.raspGPIO.HIGH)\n",
    "\n",
    "\n",
    "\n",
    "    def right_forward(self):\n",
    "        self.servo.rotateMotor('right')\n",
    "        self.raspGPIO.output(self.WLF, self.raspGPIO.HIGH)\n",
    "        self.raspGPIO.output(self.WRF, self.raspGPIO.HIGH)\n",
    "\n",
    "    def left_backward(self):\n",
    "        self.servo.rotateMotor('left')\n",
    "        self.raspGPIO.output(self.WLB, self.raspGPIO.HIGH)\n",
    "        self.raspGPIO.output(self.WRB, self.raspGPIO.HIGH)\n",
    "\n",
    "    def right_backward(self):\n",
    "        self.servo.rotateMotor('right')\n",
    "        self.raspGPIO.output(self.WLB, self.raspGPIO.HIGH)\n",
    "        self.raspGPIO.output(self.WRB, self.raspGPIO.HIGH)\n",
    "    \n",
    "    def movimentacarro(self, movimento):\n",
    "        if movimento==0:\n",
    "            self.forward()\n",
    "        elif movimento==1:\n",
    "             self.backward()\n",
    "        elif movimento==2:\n",
    "             self.left_forward()\n",
    "        elif movimento==3:\n",
    "             self.right_forward()\n",
    "        elif movimento==4:\n",
    "             self.left_backward()\n",
    "        elif movimento==5:\n",
    "             self.right_backward()\n",
    "        time.sleep(0.1)\n",
    "\n",
    "        self.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ServoMotor:\n",
    "    def __init__(self, raspGPIO):\n",
    "        self.position = ''\n",
    "        self.servoPIN = 18\n",
    "        self.raspGPIO = raspGPIO\n",
    "        self.raspGPIO.setup(self.servoPIN, GPIO.OUT)\n",
    "        self.servo = self.raspGPIO.PWM(self.servoPIN, 50) # GPIO 18 for PWM with 50Hz\n",
    "        self.servo.start(0)\n",
    "        time.sleep(2)\n",
    "\n",
    "        duty = 11\n",
    "\n",
    "        while duty >= 7:\n",
    "            self.servo.ChangeDutyCycle(duty)\n",
    "            time.sleep(0.3)\n",
    "            self.servo.ChangeDutyCycle(0)\n",
    "            time.sleep(0.7)\n",
    "            duty -= 1\n",
    "    \n",
    "    def stop(self):\n",
    "        self.servo.stop()\n",
    "    \n",
    "    def rotateMotor(self, position):\n",
    "        if 'center' in position and 'center' not in self.position :\n",
    "            print('position center')\n",
    "            self.servo.ChangeDutyCycle(9)\n",
    "            time.sleep(0.3)\n",
    "            self.servo.ChangeDutyCycle(0)\n",
    "            time.sleep(0.7)\n",
    "            self.position = 'center'\n",
    "\n",
    "\n",
    "        \n",
    "        if 'left' in position and 'left' not in self.position:\n",
    "            print('position left')\n",
    "            self.servo.ChangeDutyCycle(11)\n",
    "            time.sleep(0.3)\n",
    "            self.servo.ChangeDutyCycle(0)\n",
    "            time.sleep(0.7)            \n",
    "            self.position = 'left'\n",
    "\n",
    "\n",
    "        if 'right' in position and 'right' not in self.position:\n",
    "            print('position right')\n",
    "            self.servo.ChangeDutyCycle(7)\n",
    "            time.sleep(0.3)\n",
    "            self.servo.ChangeDutyCycle(0)\n",
    "            time.sleep(0.7)            \n",
    "            self.position = 'right'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir(base):\n",
    "    ''''\n",
    "    Cria diretórios para salvar as matrizes \n",
    "    '''\n",
    "    path = os.path.join('save', base)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    return path\n",
    "\n",
    "def salvaMatrizes(matrizNome, matrizValor):\n",
    "    ''''\n",
    "    Cria diretórios para salvar as matrizes \n",
    "    Salva as matrizes com o nome e o valor passado como parametro\n",
    "    '''\n",
    "    today=datetime.today().strftime('%Y-%m-%d')\n",
    "    mkdir(today)\n",
    "    daytime=datetime.today().strftime('%H:%M:%S')\n",
    "    np.savez(('save/%s/%s_%s.npz' % (today, matrizNome, daytime)), matrizValor)\n",
    "\n",
    "def carregaMatriz(pasta, matrizNome):\n",
    "    ''''\n",
    "    Carrega as matrizes salvas no diretório\n",
    "    '''\n",
    "    path = os.path.join('save', pasta, matrizNome)\n",
    "    load = np.load(path)\n",
    "    return load['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CarEnv:\n",
    "    def __init__(self, motorCarro, radar):\n",
    "        self.actions_space = ['forward', 'backward', 'leftforward', 'rightforward', 'leftbackward', 'rightbackward']\n",
    "        self.initialradarpositions = radar.get_distancias()\n",
    "        self.observation_space = np.array(np.zeros([len(self.initialradarpositions), len(self.actions_space)]))\n",
    "        self.state = self.initialradarpositions\n",
    "        self.done = False\n",
    "        self.input_size = len(self.initialradarpositions)\n",
    "        self.output_size = len(self.actions_space)\n",
    "        self.motorCar =  motorCarro\n",
    "        self.finishCount = 0\n",
    "        self.radar = radar\n",
    "    \n",
    "    def finish(self, state):\n",
    "        '''\n",
    "        Quando o carro se movimentar 3 vezes para a frente sem parar é o objetivo dele\n",
    "        '''\n",
    "        print(\"STATE FINISH:\", state)\n",
    "        if(state[1]>15  and state[3]>15):\n",
    "            self.finishCount+=1\n",
    "        else:\n",
    "            self.finishCount=0\n",
    "        \n",
    "        print(\"Count FINISH:\", self.finishCount)\n",
    "\n",
    "        if self.finishCount>=3:\n",
    "            self.finishCount = 0\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def step(self, action):\n",
    "        self.take_action(action)\n",
    "        self.state = self.getState()\n",
    "        self.done = self.finish(self.state)\n",
    "        stepP = copy.deepcopy(self.getReward()), self.state, self.done\n",
    "        return stepP\n",
    "\n",
    "    def getState(self):\n",
    "        try:\n",
    "            self.state = self.radar.get_distancias()\n",
    "        finally:\n",
    "           self.motorCar.movimentacarro(0)\n",
    "        return self.state\n",
    "        \n",
    "    def take_action(self, action):\n",
    "        movPosition = np.where(action == np.max(action)) \n",
    "        l = list(action).index(np.max(action))\n",
    "        print('action', l, np.max(action), action)\n",
    "        self.motorCar.movimentacarro(l)\n",
    "\n",
    "    def getReward(self):\n",
    "\n",
    "        #f = lambda x: 10 if  x>100 else -10\n",
    "        #return f(max(self.state[:5]))\n",
    "        retVal = -1\n",
    "        print('state recompensa', self.state)\n",
    "        if(self.state[0]>15  and self.state[1]>15  and self.state[2]>15  and self.state[3]>15):\n",
    "            retVal=1\n",
    "        return retVal\n"
   ]
  },
  {
   "source": [
    "### Inicialização dos Hiperparametros\n",
    "Neste parte implementaremos a seguinte parte do código\n",
    "\n",
    "<img src=\"imagens/ars_part1.png\" width=600 heigth=400>\n",
    "\n",
    "Inicialmente vamos usar steps_size=10 e epsodes=10 apenas para fins de comparação de resultados, ja que o ambiente é real de um carro autonomo\n",
    "\n",
    "self.directions = Total de matrizes de pertubações a serem contruidas ***number of directions sampled per iteration N***\n",
    "\n",
    "self.best_directions = Total de matrizes com recompensas melhores \n",
    "\n",
    "Nunca as matrizes de pertubação pode ser maior que as matrizes de recompensas, por isso o uso do assert\n",
    "\n",
    "***(number of top-performing directions to use b (b < N is allowed only for V1-t and V2-t)***\n",
    "\n",
    "self.noise = noise ν"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hiperparametros():\n",
    "    def __init__(self):\n",
    "        self.epochs = 1000\n",
    "        self.epsodes = 10\n",
    "        self.lr = 0.02\n",
    "        self.directions = 6\n",
    "        self.best_directions = 6\n",
    "        assert self.best_directions <= self.directions\n",
    "        self.noise = 0.03\n",
    "        self.seed = 1\n",
    "        self.env_nome = ''\n"
   ]
  },
  {
   "source": [
    "Os bons resultados desse algoritmo se dão fortemente por causa da normalização dos dados\n",
    "\n",
    "De acordo com documento, a normalização é necessário por:\n",
    "\n",
    "\"A normalização de estados usada por V2 é semelhante ao clareamento de dados usado em tarefas de regressão, e\n",
    "intuitivamente, garante que as políticas atribuam peso igual aos diferentes componentes dos estados. Para\n",
    "obter intuição de por que isso pode ajudar, suponha que uma coordenada de estado só tenha valores no intervalo\n",
    "90, 100 enquanto outro componente de estado assume valores na faixa -1, 1. Então, pequenas mudanças em\n",
    "o ganho de controle em relação à primeira coordenada de estado levaria a mudanças maiores nas ações\n",
    "então, o mesmo tamanho muda em relação ao segundo componente de estado. Portanto, o clareamento permite\n",
    "a exploração isotrópica de pesquisa aleatória para ter igual influência sobre os vários componentes de estado\"\n",
    "\n",
    "\n",
    "Nesse caso vamos normalizar os valores recebidos pelo radar que varia entre 1 e 899, que seriam equivalentes há 1cm e 89,9cm. \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizacao dos estados (Standardization)\n",
    "class Normalizacao():\n",
    "    def __init__(self, inputs):\n",
    "        '''\n",
    "        Inicializa todos os parametros utilizados durante a normalização\n",
    "        Keyword arguments:\n",
    "        inputs -- array dos valores recebidos do sensor de distancia, ex: [30, 6, 3, 4, 8, 91, 819]\n",
    "        '''\n",
    "        self.n = np.zeros(inputs) #agregador de estados descobertos desde o inicio\n",
    "        self.mean = np.zeros(inputs) #média de todos os valores de input\n",
    "        self.mean_diff = np.zeros(inputs) #usado para o calculo da variancia\n",
    "        self.var = np.zeros(inputs) #guarda os valores de variancia\n",
    "    \n",
    "    def observe(self, inputs):\n",
    "        '''\n",
    "        Realiza o calculo da variancia nos dados recebidos do sensor de movimento\n",
    "        Keyword arguments:\n",
    "        inputs -- array dos valores recebidos do sensor de distancia, ex: [30, 6, 3, 4, 8, 91, 819]\n",
    "        '''\n",
    "        self.n +=1. #indica em que ação é a atual\n",
    "        last_mean = self.mean.copy() #guarda o valor da ultima média realizada \n",
    "        self.mean += (inputs - self.mean) / self.n #atualizando a média baseada na quantidade de ações ja realizadas\n",
    "        self.mean_diff += (inputs - last_mean) * (inputs - self.mean) # pega a diferença atual e a média antiga\n",
    "        self.var = (self.mean_diff/self.n).clip(min = 1e-2) #realiza o calculo da variancia, e limita o valor minino em 0.01\n",
    "\n",
    "    def normalize(self, inputs):\n",
    "        ''''\n",
    "        Realiza o calculo da normalização(Padronização) dividindo realizando x- média(x) / desvio padrão de X\n",
    "        Assim deixando todos os valores dentro da escala -1 e 1\n",
    "        Essa forma é mais robusta contra outliers\n",
    "\n",
    "        Keyword arguments:\n",
    "        inputs -- array dos valores recebidos do sensor de distancia, ex: [30, 6, 3, 4, 8, 91, 819]\n",
    "\n",
    "        Return:\n",
    "        Normalized - Valores normalizados entre -1 e 1\n",
    "        '''\n",
    "        obs_mean = self.mean #Qual a média atual\n",
    "        obs_std = np.sqrt(self.var) #Calcula Desvio padrão\n",
    "        normalized = (inputs - obs_mean) / obs_std #(valor a ser nomalizado - média) / desvio padrão \n",
    "        return normalized"
   ]
  },
  {
   "source": [
    "Neste parte implementaremos as seguintes partes do código\n",
    "\n",
    "<img src=\"imagens/ars_part2.png\" width=600 heigth=400>\n",
    "\n",
    "<img src=\"imagens/ars_part3.png\" width=600 heigth=400>\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Politicas():\n",
    "    '''\n",
    "        A exploração acontece no espaço das politicas depois que todo um episódio e steps dele foram executados\n",
    "        Diferente de outras IAs que usam exploração por ambientes e ações executadas\n",
    "    '''\n",
    "\n",
    "    def __init__(self, input_size, output_size):\n",
    "        '''\n",
    "        Keyword arguments:\n",
    "        input_size -- Numeros de entradas\n",
    "        output_size -- Numeros de saídas\n",
    "\n",
    "        Return:\n",
    "        '''\n",
    "        self.theta = np.zeros((output_size, input_size)) #Cria uma matrix de pesos inicializado em zeros\n",
    "        #Estamos seguindo o método do artivo pelo lado esquerdo, por isso usamos (output_size, input_size)\n",
    "    \n",
    "    def evaluate(self, input, delta=None, direction=None):\n",
    "        ''''\n",
    "        De acordo com a direção passada em direction é atualizada a matriz de pesos\n",
    "\n",
    "        Keyword arguments:\n",
    "        inputs -- array dos valores recebidos do sensor de distancia, ex: [30, 6, 3, 4, 8, 91, 819]\n",
    "        delta -- Matrix de pertubação dos numeros \n",
    "        direction -- Indica a direção do calculo para positivo ou negativo\n",
    "        '''\n",
    "\n",
    "        if direction is None:\n",
    "            return self.theta.dot(input) #retorna a matriz de pesos que multiplica com as entradas, sem pertubações\n",
    "        elif direction == 'positive':\n",
    "            return (self.theta + hp.noise * delta).dot(input) #retorna a matriz de pesos que multiplica com as entradas, com ruido de exploração mais pertubações positivas\n",
    "        else: \n",
    "            return (self.theta - hp.noise * delta).dot(input)#retorna a matriz de pesos que multiplica com as entradas, com ruido de exploração mais pertubações negativas\n",
    "    \n",
    "    def samples_deltas(self):\n",
    "        '''\n",
    "        Gerando matrix de pertubação, matrix com numeros aleatórios\n",
    "        '''\n",
    "        return [np.random.randn(*self.theta.shape) for _ in range(hp.directions)]\n",
    "        #retorna uma matrix com distribuição normal para o todas as matrizes de pertubaçao \n",
    "    \n",
    "    def update(self, rollouts, sigma_r):\n",
    "        '''\n",
    "        Item 7 do Algoritmo, fazendo a atualização dos pesos\n",
    "\n",
    "        Keyword arguments:\n",
    "        rollouts -- conjunto de recompensa positiva, conjunto recompensa negativa e a matrizx de numeros aleatórios\n",
    "        sigma_r -- Indica o desvio padrão da recompensa\n",
    "        '''\n",
    "        step = np.zeros(self.theta.shape) #Inicializa com as dimensões de pesos\n",
    "        for r_pos, r_neg, d in rollouts:\n",
    "            step += (r_pos - r_neg) * d #Somatoria das recompensas positivas e negativas e a multicao do delta\n",
    "        self.theta += hp.lr / (hp.best_directions * sigma_r) * step #Atualizando a matriz  de pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore(env, normalizer, policy, direction = None, delta=None):\n",
    "    '''\n",
    "         Faz a exploração do ambiente enquanto não finalizar e não terminar as execuções do epsodio\n",
    "      \n",
    "    '''\n",
    "    state = env.getState() #ler o ambiente radar\n",
    "    done = False #inicia em False o objetivo\n",
    "    num_plays = 0. #contador de rodadas no episódio\n",
    "    sum_rewards = 0 #soma das recompensas\n",
    "    while not done or num_plays < hp.epsodes:\n",
    "       normalizer.observe(state) #Atualiza o calculo da variancia nos dados recebidos do sensor de movimento\n",
    "       state = normalizer.normalize(state) #Realiza o calculo da normalização(Padronização) deixando todos os estados entre -1 e 1\n",
    "       action = policy.evaluate(state, delta, direction) #atualizada a matriz de pesos de acordo com a direçao selecionada e retirna \n",
    "       reward, state, done = env.step(action) #Executa a ação selecionada e retirna a nova leitura do ambiente e se foi finalizado \n",
    "       print('Execucao: ', num_plays, ', Episodio: ', hp.epsodes, ' Finalizado: ', done, \"Recompensa: \", reward)\n",
    "       reward = max(min(reward, 1), -1)#evita outlier nas recompensas\n",
    "       sum_rewards += reward #Soma das recompensas\n",
    "       num_plays +=1 #atualida o numero da rodada\n",
    "    return sum_rewards\n"
   ]
  },
  {
   "source": [
    "Neste parte implementaremos as seguintes partes do código\n",
    "\n",
    "<img src=\"imagens/ars_part4.png\" width=600 heigth=400>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Treinando\n",
    "def train(env, policy, normalizer, hp):\n",
    "    ''''\n",
    "      Realiza o treinamento da rede\n",
    "   \n",
    "    '''\n",
    "    loadMatrixPositiveFilename = None\n",
    "    LoadMatrixFolder = None\n",
    "    loadMatrixNegativeFilename = None\n",
    "    DeltaFilename = None\n",
    "    for step in range(hp.epochs):\n",
    "        \n",
    "        if loadMatrixPositiveFilename and loadMatrixNegativeFilename and LoadMatrixFolder and DeltaFilename:\n",
    "            deltas = carregaMatriz(LoadMatrixFolder, loadMatrixPositiveFilename) #Inicializacao das pertubacoes (deltas) e as recompensas negativas e positivas)\n",
    "            #positive_rewards = \n",
    "            negative_rewards = carregaMatriz(LoadMatrixFolder, DeltaFilename)\n",
    "        else:\n",
    "            deltas = policy.samples_deltas() #Inicializacao das pertubacoes (deltas) e as recompensas negativas e positivas)\n",
    "            positive_rewards = [0] * hp.directions #inicializando a matriz de recompensas positivas\n",
    "            negative_rewards = [0] * hp.directions #inicializando a matriz de recompensas negativas\n",
    "\n",
    "        #obtendo as recompensas na direcao positiva\n",
    "        for k in range(hp.directions):\n",
    "            positive_rewards[k] = explore(env, normalizer, policy, direction='positive', delta=deltas[k])\n",
    "            \n",
    "        #obtendo recompensa na direcao negativa\n",
    "        for k in range(hp.directions):\n",
    "            positive_rewards[k] = explore(env, normalizer, policy,  direction='negative', delta=deltas[k])\n",
    "        \n",
    "        #obtendo todas as recompensas positivas e negativas para computar o desvio dessas recompensas\n",
    "        all_reward = np.array(positive_rewards + negative_rewards)\n",
    "        sigma_r = all_reward.std()\n",
    "\n",
    "        #ordenacao dos rollouts e selecao das melhores direcoes\n",
    "        scores = {k: max(r_pos, r_neg) for k, (r_pos, r_neg) in enumerate(zip(positive_rewards, negative_rewards))}\n",
    "        order = sorted(scores.keys(), key=lambda x: scores[x], reverse=True)[:hp.best_directions]\n",
    "        rollouts = [(positive_rewards[k], negative_rewards[k], deltas [k]) for k in order]\n",
    "\n",
    "        #atualizacao de politica\n",
    "        policy.update(rollouts, sigma_r)\n",
    "\n",
    "        #impressao da recompensa\n",
    "        reward_evaluation = explore(env, normalizer, policy)\n",
    "        print('Step', step, 'Reward:',reward_evaluation )\n",
    "\n",
    "    salvaMatrizes('positive_rewards', positive_rewards)\n",
    "    salvaMatrizes('negative_rewards', negative_rewards)\n",
    "    alvaMatrizes('deltas', deltas)\n",
    "\n",
    "    \n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[36, 5, 4, 253]\ncenter\nposition center\nleft\nposition left\nright\nposition right\nleft\nposition left\nright\nposition right\ncenter\nposition center\n[101, 69, 41, 31]\n[80, 97, 43, 32]\n"
    }
   ],
   "source": [
    "\n",
    "\n",
    "raspGPIO  = SetupGPIO()\n",
    "raspGPIO = raspGPIO.get_gpio()\n",
    "radar = radar_new(raspGPIO)\n",
    "radar.initialize()\n",
    "servo = ServoMotor(raspGPIO)\n",
    "motorCar = MotorCarro(raspGPIO, servo)\n",
    "print(radar.get_distancias())\n",
    "motorCar.movimentacarro(1)\n",
    "motorCar.movimentacarro(2)\n",
    "motorCar.movimentacarro(3)\n",
    "motorCar.movimentacarro(4)\n",
    "motorCar.movimentacarro(5)\n",
    "motorCar.movimentacarro(0)\n",
    "print(radar.get_distancias())\n",
    "\n",
    "\n",
    "carEnv = CarEnv(motorCar, radar)\n",
    "print(radar.get_distancias())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "on center\nSTATE FINISH: [19, 49, 16, 70]\nCount FINISH: 7\nstate recompensa [19, 49, 16, 70]\nRecompensa:  1\ncenter\nExecucao:  0.0 , Episodio:  100  Finalizado:  False\naction 4 0.05431477030425723 [-0.00237343 -0.09203562 -0.06636076 -0.04798032  0.05431477 -0.00663035]\nleft\nposition left\ncenter\nposition center\nSTATE FINISH: [24, 18, 16, 173]\nCount FINISH: 8\nstate recompensa [24, 18, 16, 173]\nRecompensa:  1\ncenter\nExecucao:  0.0 , Episodio:  100  Finalizado:  False\naction 1 0.1750372912252933 [ 0.02531335  0.17503729  0.15053458  0.12051334  0.02333853 -0.03576425]\ncenter\ncenter\nSTATE FINISH: [13, 72, 18, 182]\nCount FINISH: 9\nstate recompensa [13, 72, 18, 182]\nRecompensa:  -1\ncenter\nExecucao:  0.0 , Episodio:  100  Finalizado:  False\naction 1 0.06303228754374054 [ 0.010479    0.06303229  0.01448019 -0.09461202 -0.03552318 -0.08962757]\ncenter\ncenter\nSTATE FINISH: [72, 30, 29, 197]\nCount FINISH: 10\nstate recompensa [72, 30, 29, 197]\nRecompensa:  1\ncenter\nExecucao:  0.0 , Episodio:  100  Finalizado:  False\naction 5 0.1399948345606427 [-0.02566553 -0.02177344  0.07865193 -0.08118075  0.08159784  0.13999483]\nright\nposition right\ncenter\nposition center\nSTATE FINISH: [26, 21, 35, 58]\nCount FINISH: 11\nstate recompensa [26, 21, 35, 58]\nRecompensa:  1\ncenter\nExecucao:  0.0 , Episodio:  100  Finalizado:  False\naction 4 0.07751016147750057 [-0.031773    0.06107249 -0.00146117  0.02602787  0.07751016 -0.04240666]\nleft\nposition left\ncenter\nposition center\nSTATE FINISH: [14, 55, 35, 17]\nCount FINISH: 12\nstate recompensa [14, 55, 35, 17]\nRecompensa:  -1\ncenter\nExecucao:  0.0 , Episodio:  100  Finalizado:  False\naction 5 0.05235239000199423 [ 0.01501724 -0.00194944 -0.04545505 -0.0042798  -0.019105    0.05235239]\nright\nposition right\ncenter\nposition center\nSTATE FINISH: [34, 39, 20, 22]\nCount FINISH: 13\nstate recompensa [34, 39, 20, 22]\nRecompensa:  1\ncenter\nExecucao:  0.0 , Episodio:  100  Finalizado:  False\naction 0 0.016616578505524386 [ 0.01661658 -0.04348689 -0.01702969 -0.04468572 -0.01983911 -0.00457799]\ncenter\ncenter\nSTATE FINISH: [91, 79, 40, 6]\nCount FINISH: 0\nstate recompensa [91, 79, 40, 6]\nRecompensa:  -1\nExecucao:  1.0 , Episodio:  100  Finalizado:  False\naction 0 0.0930797544423502 [ 0.09307975  0.00060369 -0.01119801 -0.0163732   0.01700794  0.02584859]\ncenter\ncenter\nSTATE FINISH: [43, 46, 50, 40]\nCount FINISH: 1\nstate recompensa [43, 46, 50, 40]\nRecompensa:  1\nExecucao:  2.0 , Episodio:  100  Finalizado:  False\naction 0 -0.012460330291581508 [-0.01246033 -0.08074754 -0.01933113 -0.06061293 -0.02770731 -0.02219046]\ncenter\ncenter\nSTATE FINISH: [53, 50, 51, 52]\nCount FINISH: 2\nstate recompensa [53, 50, 51, 52]\nRecompensa:  1\nExecucao:  3.0 , Episodio:  100  Finalizado:  False\naction 2 -0.00580192094348455 [-0.02048691 -0.06625232 -0.00580192 -0.04338745 -0.02629939 -0.02380468]\nleft\nposition left\ncenter\nposition center\nSTATE FINISH: [1208, 1212, 1209, 128]\nCount FINISH: 3\nstate recompensa [1208, 1212, 1209, 128]\nRecompensa:  1\ncenter\nExecucao:  0.0 , Episodio:  100  Finalizado:  False\naction 0 0.015361404872014772 [ 0.0153614   0.00610421 -0.04174608  0.0058949  -0.01231639 -0.00059239]\ncenter\ncenter\nSTATE FINISH: [1208, 1212, 278, 134]\nCount FINISH: 4\nstate recompensa [1208, 1212, 278, 134]\nRecompensa:  1\nStep 0 Reward: 1\ncenter\nExecucao:  0.0 , Episodio:  100  Finalizado:  False\naction 2 0.2956778481882522 [ 0.1176119  -0.02436572  0.29567785 -0.04057479 -0.11203784  0.16178425]\nleft\nposition left\ncenter\nposition center\nSTATE FINISH: [1208, 1212, 102, 158]\nCount FINISH: 5\nstate recompensa [1208, 1212, 102, 158]\nRecompensa:  1\ncenter\nExecucao:  0.0 , Episodio:  100  Finalizado:  False\naction 1 0.1680494121973515 [-0.08920566  0.16804941  0.11013527 -0.06305793 -0.02757067  0.07772285]\ncenter\ncenter\nSTATE FINISH: [1208, 221, 19, 140]\nCount FINISH: 6\nstate recompensa [1208, 221, 19, 140]\nRecompensa:  1\ncenter\nExecucao:  0.0 , Episodio:  100  Finalizado:  False\naction 5 0.16318194718625303 [ 0.06433174  0.02280039  0.03557288 -0.13413091 -0.09634307  0.16318195]\nright\nposition right\ncenter\nposition center\nSTATE FINISH: [1208, 1212, 20, 141]\nCount FINISH: 7\nstate recompensa [1208, 1212, 20, 141]\nRecompensa:  1\ncenter\nExecucao:  0.0 , Episodio:  100  Finalizado:  False\naction 0 0.044307247029239896 [ 0.04430725  0.03406955 -0.09198761 -0.06897598  0.0094208   0.02856373]\ncenter\ncenter\nSTATE FINISH: [779, 1212, 1209, 165]\nCount FINISH: 8\nstate recompensa [779, 1212, 1209, 165]\nRecompensa:  1\ncenter\nExecucao:  0.0 , Episodio:  100  Finalizado:  False\naction 4 0.10742485677515727 [-0.08767087  0.0043194  -0.01043851 -0.05476569  0.10742486 -0.09810995]\nleft\nposition left\ncenter\nposition center\nSTATE FINISH: [1207, 1212, 183, 1208]\nCount FINISH: 9\nstate recompensa [1207, 1212, 183, 1208]\nRecompensa:  1\ncenter\nExecucao:  0.0 , Episodio:  100  Finalizado:  False\naction 0 0.09527878370133273 [ 0.09527878  0.03623494  0.05910045  0.05543181 -0.04067234  0.02395528]\ncenter\ncenter\nSTATE FINISH: [173, 1212, 86, 179]\nCount FINISH: 10\nstate recompensa [173, 1212, 86, 179]\nRecompensa:  1\ncenter\nExecucao:  0.0 , Episodio:  100  Finalizado:  False\naction 1 0.127264635408537 [-0.00149615  0.12726464 -0.05750214  0.07943908  0.03066523 -0.00092263]\ncenter\ncenter\nSTATE FINISH: [181, 1212, 204, 213]\nCount FINISH: 11\nstate recompensa [181, 1212, 204, 213]\nRecompensa:  1\ncenter\nExecucao:  0.0 , Episodio:  100  Finalizado:  False\naction 1 0.10331875064607027 [ 0.09741789  0.10331875  0.07606838 -0.05209759 -0.00164353 -0.01409702]\ncenter\ncenter\nSTATE FINISH: [178, 1212, 180, 221]\nCount FINISH: 12\nstate recompensa [178, 1212, 180, 221]\nRecompensa:  1\ncenter\nExecucao:  0.0 , Episodio:  100  Finalizado:  False\naction 0 0.2672923778179004 [ 0.26729238 -0.06524777 -0.25789245  0.1832001   0.04327926 -0.06443463]\ncenter\ncenter\nSTATE FINISH: [1208, 1212, 39, 33]\nCount FINISH: 13\nstate recompensa [1208, 1212, 39, 33]\nRecompensa:  1\ncenter\nExecucao:  0.0 , Episodio:  100  Finalizado:  False\naction 5 0.201952444009934 [-0.10742543 -0.20854178 -0.0350023   0.03209376 -0.10565818  0.20195244]\nright\nposition right\ncenter\nposition center\nSTATE FINISH: [173, 1212, 200, 1208]\nCount FINISH: 14\nstate recompensa [173, 1212, 200, 1208]\nRecompensa:  1\ncenter\nExecucao:  0.0 , Episodio:  100  Finalizado:  False\naction 0 0.3257617216508927 [0.32576172 0.09218292 0.13871879 0.06169834 0.08527698 0.14348098]\ncenter\ncenter\nSTATE FINISH: [143, 1212, 73, 114]\nCount FINISH: 15\nstate recompensa [143, 1212, 73, 114]\nRecompensa:  1\ncenter\nExecucao:  0.0 , Episodio:  100  Finalizado:  False\naction 4 0.049524609486794616 [-0.07261271 -0.01592138 -0.05556897  0.03287102  0.04952461 -0.01431895]\nleft\nposition left\ncenter\nposition center\nSTATE FINISH: [146, 1212, 188, 246]\nCount FINISH: 16\nstate recompensa [146, 1212, 188, 246]\nRecompensa:  1\ncenter\nExecucao:  0.0 , Episodio:  100  Finalizado:  False\naction 0 0.11600416927010704 [ 0.11600417  0.06478004 -0.05932567  0.03018333 -0.01348554  0.10903869]\ncenter\ncenter\nSTATE FINISH: [111, 1212, 182, 100]\nCount FINISH: 17\nstate recompensa [111, 1212, 182, 100]\nRecompensa:  1\nStep 1 Reward: 1\ncenter\nExecucao:  0.0 , Episodio:  100  Finalizado:  False\naction 2 0.06975984394360955 [-0.011011    0.0016864   0.06975984 -0.01665796  0.06755162 -0.10438653]\nleft\nposition left\ncenter\nposition center\nSTATE FINISH: [111, 1212, 182, 99]\nCount FINISH: 18\nstate recompensa [111, 1212, 182, 99]\nRecompensa:  1\ncenter\nExecucao:  0.0 , Episodio:  100  Finalizado:  False\naction 0 0.19247578347147237 [0.19247578 0.04153375 0.01024782 0.03320243 0.09037379 0.06647356]\ncenter\ncenter\nSTATE FINISH: [112, 1212, 37, 100]\nCount FINISH: 19\nstate recompensa [112, 1212, 37, 100]\nRecompensa:  1\ncenter\nExecucao:  0.0 , Episodio:  100  Finalizado:  False\naction 1 0.08079759578765752 [-4.19574293e-02  8.07975958e-02  5.08311833e-02 -5.10423367e-05\n  3.30580011e-02 -6.30885771e-02]\ncenter\ncenter\nSTATE FINISH: [118, 121, 1210, 101]\nCount FINISH: 20\nstate recompensa [118, 121, 1210, 101]\nRecompensa:  1\ncenter\nExecucao:  0.0 , Episodio:  100  Finalizado:  False\naction 5 0.027336362750426682 [ 0.00331086 -0.06926901 -0.04474452 -0.01153663  0.00360148  0.02733636]\nright\nposition right\ncenter\nposition center\nSTATE FINISH: [115, 141, 565, 76]\nCount FINISH: 21\nstate recompensa [115, 141, 565, 76]\nRecompensa:  1\ncenter\nExecucao:  0.0 , Episodio:  100  Finalizado:  False\naction 5 0.034503598991682274 [-0.02494029 -0.02434224 -0.04217281 -0.00316565 -0.0032871   0.0345036 ]\nright\nposition right\ncenter\nposition center\nSTATE FINISH: [82, 88, 91, 304]\nCount FINISH: 22\nstate recompensa [82, 88, 91, 304]\nRecompensa:  1\ncenter\nExecucao:  0.0 , Episodio:  100  Finalizado:  False\naction 5 0.04243059652495633 [-0.0061911  -0.06391411 -0.07753755 -0.01703391 -0.03992411  0.0424306 ]\nright\nposition right\ncenter\nposition center\nSTATE FINISH: [54, 62, 64, 334]\nCount FINISH: 23\nstate recompensa [54, 62, 64, 334]\nRecompensa:  1\ncenter\nExecucao:  0.0 , Episodio:  100  Finalizado:  False\naction 5 0.039288943059758684 [-0.02947445 -0.04005253 -0.02704198  0.03411207  0.03096707  0.03928894]\nright\nposition right\ncenter\nposition center\nSTATE FINISH: [37, 31, 29, 364]\nCount FINISH: 24\nstate recompensa [37, 31, 29, 364]\nRecompensa:  1\ncenter\nExecucao:  0.0 , Episodio:  100  Finalizado:  False\naction 5 0.049121578704460975 [ 0.03204336 -0.02065023 -0.02985147  0.02608572 -0.01158686  0.04912158]\nright\nposition right\ncenter\nposition center\nSTATE FINISH: [1208, 1212, 17, 46]\nCount FINISH: 25\nstate recompensa [1208, 1212, 17, 46]\nRecompensa:  1\ncenter\nExecucao:  0.0 , Episodio:  100  Finalizado:  False\naction 0 0.12558984513221097 [ 0.12558985 -0.00082865  0.07503635 -0.04189661 -0.02688722 -0.04447944]\ncenter\ncenter\nSTATE FINISH: [29, 1223, 1209, 120]\nCount FINISH: 26\nstate recompensa [29, 1223, 1209, 120]\nRecompensa:  1\ncenter\nExecucao:  0.0 , Episodio:  100  Finalizado:  False\naction 4 0.23016919810369763 [ 0.12012491  0.00982745  0.00353135 -0.01957833  0.2301692   0.06759057]\nleft\nposition left\ncenter\nposition center\nSTATE FINISH: [41, 19, 20, 106]\nCount FINISH: 27\nstate recompensa [41, 19, 20, 106]\nRecompensa:  1\ncenter\nExecucao:  0.0 , Episodio:  100  Finalizado:  False\naction 3 0.030228563102229888 [-0.02255562 -0.07847386 -0.03679378  0.03022856 -0.00347471 -0.0320413 ]\nright\nposition right\ncenter\nposition center\nSTATE FINISH: [90, 1212, 1209, 113]\nCount FINISH: 28\nstate recompensa [90, 1212, 1209, 113]\nRecompensa:  1\ncenter\nExecucao:  0.0 , Episodio:  100  Finalizado:  False\naction 1 0.28816296248075257 [ 0.01981535  0.28816296 -0.08385161  0.03205816  0.09132836 -0.12041405]\ncenter\ncenter\nSTATE FINISH: [99, 100, 22, 100]\nCount FINISH: 29\nstate recompensa [99, 100, 22, 100]\nRecompensa:  1\ncenter\nExecucao:  0.0 , Episodio:  100  Finalizado:  False\naction 3 0.07011422883496696 [-0.02530311  0.03601129  0.02531742  0.07011423 -0.00297088 -0.07552074]\nright\nposition right\ncenter\nposition center\nSTATE FINISH: [85, 1212, 384, 41]\nCount FINISH: 30\nstate recompensa [85, 1212, 384, 41]\nRecompensa:  1\nStep 2 Reward: 1\ncenter\nExecucao:  0.0 , Episodio:  100  Finalizado:  False\naction 1 0.09897810452668208 [ 0.0417963   0.0989781  -0.13802143  0.02842486  0.01440478 -0.02873164]\ncenter\ncenter\nSTATE FINISH: [95, 214, 19, 103]\nCount FINISH: 31\nstate recompensa [95, 214, 19, 103]\nRecompensa:  1\ncenter\nExecucao:  0.0 , Episodio:  100  Finalizado:  False\naction 3 0.08433478946260556 [-0.01105438  0.05973382  0.02385661  0.08433479  0.04422774 -0.05376792]\nright\nposition right\ncenter\nposition center\nSTATE FINISH: [84, 1212, 1209, 120]\nCount FINISH: 32\nstate recompensa [84, 1212, 1209, 120]\nRecompensa:  1\ncenter\nExecucao:  0.0 , Episodio:  100  Finalizado:  False\naction 1 0.07995179859384327 [-0.11076542  0.0799518  -0.04603578  0.00895682 -0.15010735 -0.07871373]\ncenter\ncenter\nSTATE FINISH: [93, 35, 13, 105]\nCount FINISH: 33\nstate recompensa [93, 35, 13, 105]\nRecompensa:  -1\ncenter\nExecucao:  0.0 , Episodio:  100  Finalizado:  False\naction 3 0.05830828222114023 [-0.0133122   0.02918431  0.02719255  0.05830828 -0.04235558 -0.07342093]\nright\nposition right\ncenter\nposition center\nSTATE FINISH: [79, 1212, 1210, 123]\nCount FINISH: 34\nstate recompensa [79, 1212, 1210, 123]\nRecompensa:  1\ncenter\nExecucao:  0.0 , Episodio:  100  Finalizado:  False\naction 0 0.06224337128890514 [ 0.06224337  0.04185021 -0.03646689 -0.0080373  -0.03393586 -0.1142783 ]\ncenter\ncenter\nSTATE FINISH: [79, 1212, 1209, 123]\nCount FINISH: 35\nstate recompensa [79, 1212, 1209, 123]\nRecompensa:  1\ncenter\nExecucao:  0.0 , Episodio:  100  Finalizado:  False\naction 3 0.1863124826850869 [-0.09685931  0.09143932  0.03017495  0.18631248  0.03063277 -0.01181344]\nright\nposition right\ncenter\nposition center\nSTATE FINISH: [79, 1212, 1209, 123]\nCount FINISH: 36\nstate recompensa [79, 1212, 1209, 123]\nRecompensa:  1\ncenter\nExecucao:  0.0 , Episodio:  100  Finalizado:  False\naction 2 0.13974963982245625 [-0.06155249 -0.03270422  0.13974964  0.09989182 -0.01658506 -0.09460452]\nleft\nposition left\ncenter\nposition center\nSTATE FINISH: [79, 1212, 1209, 124]\nCount FINISH: 37\nstate recompensa [79, 1212, 1209, 124]\nRecompensa:  1\ncenter\nExecucao:  0.0 , Episodio:  100  Finalizado:  False\naction 3 0.048344118533289904 [-0.01748394 -0.00090405 -0.0012102   0.04834412 -0.04182594 -0.07372371]\nright\nposition right\ncenter\nposition center\nSTATE FINISH: [79, 1212, 1200, 29]\nCount FINISH: 38\nstate recompensa [79, 1212, 1200, 29]\nRecompensa:  1\ncenter\nExecucao:  0.0 , Episodio:  100  Finalizado:  False\naction 4 0.12797326330406147 [ 0.06978843 -0.02224314  0.06184429  0.11758553  0.12797326 -0.04571329]\nleft\nposition left\ncenter\nposition center\nSTATE FINISH: [89, 82, 14, 110]\nCount FINISH: 39\nstate recompensa [89, 82, 14, 110]\nRecompensa:  -1\ncenter\nExecucao:  0.0 , Episodio:  100  Finalizado:  False\naction 5 0.0838167379364026 [-0.01991883  0.01668558 -0.0576462   0.01014879 -0.00108902  0.08381674]\nright\nposition right\ncenter\nposition center\nSTATE FINISH: [93, 75, 13, 121]\nCount FINISH: 40\nstate recompensa [93, 75, 13, 121]\nRecompensa:  -1\ncenter\nExecucao:  0.0 , Episodio:  100  Finalizado:  False\naction 0 0.07019921691628184 [ 0.07019922 -0.03458184 -0.06926292 -0.05388    -0.10936645  0.00654776]\ncenter\ncenter\nSTATE FINISH: [94, 81, 74, 130]\nCount FINISH: 41\nstate recompensa [94, 81, 74, 130]\nRecompensa:  1\ncenter\nExecucao:  0.0 , Episodio:  100  Finalizado:  False\naction 3 0.1106051476326071 [-0.03557873 -0.02812765 -0.01832306  0.11060515 -0.05921243  0.09730911]\nright\nposition right\ncenter\nposition center\nSTATE FINISH: [56, 58, 60, 322]\nCount FINISH: 42\nstate recompensa [56, 58, 60, 322]\nRecompensa:  1\ncenter\nExecucao:  0.0 , Episodio:  100  Finalizado:  False\naction 5 0.031727356950948166 [-0.03151561 -0.02292161 -0.03832502 -0.02214377 -0.0333869   0.03172736]\nright\nposition right\ncenter\nposition center\nSTATE FINISH: [59, 57, 65, 81]\nCount FINISH: 43\nstate recompensa [59, 57, 65, 81]\nRecompensa:  1\nStep 3 Reward: 1\ncenter\nExecucao:  0.0 , Episodio:  100  Finalizado:  False\naction 3 0.06517997183595606 [-0.0418621   0.00647947 -0.08059491  0.06517997 -0.10716002 -0.01186283]\nright\nposition right\ncenter\nposition center\nSTATE FINISH: [40, 75, 76, 31]\nCount FINISH: 44\nstate recompensa [40, 75, 76, 31]\nRecompensa:  1\ncenter\nExecucao:  0.0 , Episodio:  100  Finalizado:  False\naction 5 0.05678775861179636 [-0.00453327  0.0118414  -0.08428151  0.0313264  -0.02777812  0.05678776]\nright\nposition right\ncenter\nposition center\nSTATE FINISH: [45, 53, 80, 28]\nCount FINISH: 45\nstate recompensa [45, 53, 80, 28]\nRecompensa:  1\ncenter\nExecucao:  0.0 , Episodio:  100  Finalizado:  False\naction 5 0.07690086478681839 [-0.06410499  0.01521642 -0.03899144 -0.00563184 -0.07565302  0.07690086]\nright\nposition right\ncenter\nposition center\nSTATE FINISH: [54, 57, 57, 49]\nCount FINISH: 46\nstate recompensa [54, 57, 57, 49]\nRecompensa:  1\ncenter\nExecucao:  0.0 , Episodio:  100  Finalizado:  False\naction 3 0.07858190325480839 [-0.0598331  -0.06863914 -0.03126351  0.0785819  -0.04152676  0.07200567]\nright\nposition right\ncenter\nposition center\nSTATE FINISH: [38, 63, 170, 36]\nCount FINISH: 47\nstate recompensa [38, 63, 170, 36]\nRecompensa:  1\ncenter\nExecucao:  0.0 , Episodio:  100  Finalizado:  False\naction 5 0.028435795116561278 [-0.05045001 -0.0330355  -0.06921418 -0.02329689 -0.03900904  0.0284358 ]\nright\nposition right\ncenter\nposition center\nSTATE FINISH: [37, 40, 15, 25]\nCount FINISH: 48\nstate recompensa [37, 40, 15, 25]\nRecompensa:  -1\ncenter\nExecucao:  0.0 , Episodio:  100  Finalizado:  False\naction 3 0.0761427933125776 [-0.05557176 -0.06431907 -0.02807533  0.07614279 -0.06764558  0.00779492]\nright\nposition right\ncenter\nposition center\nSTATE FINISH: [20, 38, 32, 36]\nCount FINISH: 49\nstate recompensa [20, 38, 32, 36]\nRecompensa:  1\ncenter\nExecucao:  0.0 , Episodio:  100  Finalizado:  False\naction 5 0.07898326609581875 [-0.01771752 -0.04553266  0.00219548 -0.10367438  0.04770833  0.07898327]\nright\nposition right\ncenter\nposition center\nSTATE FINISH: [46, 48, 20, 21]\nCount FINISH: 50\nstate recompensa [46, 48, 20, 21]\nRecompensa:  1\ncenter\nExecucao:  0.0 , Episodio:  100  Finalizado:  False\naction 2 0.009062366520677264 [-0.06649734 -0.04729567  0.00906237 -0.0866086  -0.03573253  0.00730457]\nleft\nposition left\ncenter\nposition center\nSTATE FINISH: [31, 23, 147, 44]\nCount FINISH: 51\nstate recompensa [31, 23, 147, 44]\nRecompensa:  1\ncenter\nExecucao:  0.0 , Episodio:  100  Finalizado:  False\naction 4 0.007119644024885394 [-0.00030701 -0.05720473 -0.04174638 -0.0368255   0.00711964 -0.01315593]\nleft\nposition left\ncenter\nposition center\nSTATE FINISH: [22, 42, 36, 35]\nCount FINISH: 52\nstate recompensa [22, 42, 36, 35]\nRecompensa:  1\ncenter\nExecucao:  0.0 , Episodio:  100  Finalizado:  False\naction 1 0.02748550740460497 [-0.00662     0.02748551 -0.04328704 -0.11754145 -0.01760279 -0.00954235]\ncenter\ncenter\nSTATE FINISH: [30, 38, 77, 59]\nCount FINISH: 53\nstate recompensa [30, 38, 77, 59]\nRecompensa:  1\ncenter\nExecucao:  0.0 , Episodio:  100  Finalizado:  False\naction 5 0.019009892622175915 [-0.00972891 -0.00993109 -0.04525539 -0.0109425  -0.03217966  0.01900989]\nright\nposition right\ncenter\nposition center\nSTATE FINISH: [18, 41, 10, 57]\nCount FINISH: 54\nstate recompensa [18, 41, 10, 57]\nRecompensa:  -1\ncenter\nExecucao:  0.0 , Episodio:  100  Finalizado:  False\naction 5 0.04073991627608684 [-0.01392045  0.01470812 -0.04498327 -0.09263326 -0.0011888   0.04073992]\nright\nposition right\ncenter\nposition center\nSTATE FINISH: [28, 66, 117, 20]\nCount FINISH: 55\nstate recompensa [28, 66, 117, 20]\nRecompensa:  1\ncenter\nExecucao:  0.0 , Episodio:  100  Finalizado:  False\naction 3 0.038224465439497965 [-0.03751769 -0.01631444 -0.04484481  0.03822447 -0.05870704  0.02561286]\nright\nposition right\ncenter\nposition center\nSTATE FINISH: [28, 65, 116, 17]\nCount FINISH: 56\nstate recompensa [28, 65, 116, 17]\nRecompensa:  1\nStep 4 Reward: 1\ncenter\nExecucao:  0.0 , Episodio:  100  Finalizado:  False\naction 5 0.030226663430456713 [-0.02115991 -0.00302376 -0.0870929   0.02470738  0.00819484  0.03022666]\nright\nposition right\ncenter\nposition center\nSTATE FINISH: [28, 64, 100, 73]\nCount FINISH: 57\nstate recompensa [28, 64, 100, 73]\nRecompensa:  1\ncenter\nExecucao:  0.0 , Episodio:  100  Finalizado:  False\naction 3 0.07233198349460118 [-0.02441983 -0.02275247 -0.04811235  0.07233198 -0.07046035  0.04968884]\nright\nposition right\ncenter\nposition center\nSTATE FINISH: [29, 67, 118, 22]\nCount FINISH: 58\nstate recompensa [29, 67, 118, 22]\nRecompensa:  1\ncenter\nExecucao:  0.0 , Episodio:  100  Finalizado:  False\naction 3 0.047895593661151495 [-0.0090865  -0.04189315  0.01845604  0.04789559 -0.02143379  0.03270594]\nright\nposition right\ncenter\nposition center\nSTATE FINISH: [29, 65, 96, 20]\nCount FINISH: 59\nstate recompensa [29, 65, 96, 20]\nRecompensa:  1\ncenter\nExecucao:  0.0 , Episodio:  100  Finalizado:  False\naction 3 0.021168863331052355 [-0.03709622 -0.01800219 -0.04684571  0.02116886 -0.06523348 -0.00192999]\nright\nposition right\ncenter\nposition center\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-196-794c8483e95e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpolice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPoliticas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcarEnv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcarEnv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnormalizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNormalizacao\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcarEnv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcarEnv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpolice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnormalizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-194-129e2f03b49e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(env, policy, normalizer, hp)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m#obtendo as recompensas na direcao positiva\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirections\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mpositive_rewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexplore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'positive'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeltas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m#obtendo recompensa na direcao negativa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-193-5014cfc8935a>\u001b[0m in \u001b[0;36mexplore\u001b[0;34m(env, normalizer, policy, direction, delta)\u001b[0m\n\u001b[1;32m     13\u001b[0m        \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Realiza o calculo da normalização(Padronização) deixando todos os estados entre -1 e 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m        \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirection\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#atualizada a matriz de pesos de acordo com a direçao selecionada e retirna\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m        \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Executa a ação selecionada e retirna a nova leitura do ambiente e se foi finalizado\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m        \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Recompensa: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m        \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#evita outlier nas recompensas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-189-ec403dff4bd7>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mstepP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetReward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-189-ec403dff4bd7>\u001b[0m in \u001b[0;36mgetState\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mradar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_distancias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmotorCar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmovimentacarro\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-186-2d1878bc9438>\u001b[0m in \u001b[0;36mmovimentacarro\u001b[0;34m(self, movimento)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmovimentacarro\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmovimento\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmovimento\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmovimento\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m              \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-186-2d1878bc9438>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mservo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrotateMotor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'center'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraspGPIO\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWLF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraspGPIO\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHIGH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraspGPIO\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWRF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraspGPIO\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHIGH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-187-f0ade58650f0>\u001b[0m in \u001b[0;36mrotateMotor\u001b[0;34m(self, position)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mservo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mChangeDutyCycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'center'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "hp = Hiperparametros()\n",
    "np.random.seed(hp.seed)\n",
    "police = Politicas(carEnv.input_size, carEnv.output_size)\n",
    "normalizer = Normalizacao(carEnv.input_size)\n",
    "train(carEnv,police,normalizer,hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit ('3.6.9')",
   "metadata": {
    "interpreter": {
     "hash": "85595fc6e4c66cea5298aa77223b4d67754825b590a0f7ffb3f639c26cc9de1a"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}