{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!cat bibliotecas.txt | xargs -n 1 pip3 install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importação de bibliotecas\n",
    "import RPi.GPIO as GPIO\n",
    "import time, copy\n",
    "import board, busio, adafruit_vl53l0x\n",
    "import serial\n",
    "import numpy as np\n",
    "from numpy import save\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup das portas logicas do Raspibery PI\n",
    "class SetupGPIO:\n",
    "    def __init__(self):\n",
    "        self.objGPIO = GPIO\n",
    "        self.objGPIO.cleanup() # limpa todos os estados de todas as portas\n",
    "        self.objGPIO.setmode(GPIO.BCM) #Definindi uso dos numeros das portas por canais\n",
    "    \n",
    "    def get_gpio(self):\n",
    "        return self.objGPIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MotorCarro:\n",
    "    def __init__(self, raspGPIO):\n",
    "        #GPIO Rodas dianteiras\n",
    "        self.RDEF = 20\n",
    "        self.RDDF = 5\n",
    "        self.RDER = 21\n",
    "        self.RDDR = 27\n",
    "\n",
    "        #GPIO Rodas dianteiras\n",
    "        self.RTEF = 12\n",
    "        self.RTDF = 22\n",
    "        self.RTER = 16\n",
    "        self.RTDR = 17\n",
    "        self.raspGPIO = raspGPIO\n",
    "        self.raspGPIO.setup(self.RDEF, self.raspGPIO.OUT)\n",
    "        self.raspGPIO.setup(self.RDDF, self.raspGPIO.OUT)\n",
    "        self.raspGPIO.setup(self.RDER, self.raspGPIO.OUT)\n",
    "        self.raspGPIO.setup(self.RDDR, self.raspGPIO.OUT)\n",
    "        self.raspGPIO.setup(self.RTEF, self.raspGPIO.OUT)\n",
    "        self.raspGPIO.setup(self.RTDF, self.raspGPIO.OUT)\n",
    "        self.raspGPIO.setup(self.RTER, self.raspGPIO.OUT)\n",
    "        self.raspGPIO.setup(self.RTDR, self.raspGPIO.OUT)\n",
    "        self.stop()\n",
    "\n",
    "    def stop(self):\n",
    "        self.raspGPIO.output(self.RDEF, self.raspGPIO.LOW)\n",
    "        self.raspGPIO.output(self.RDDF, self.raspGPIO.LOW)\n",
    "        self.raspGPIO.output(self.RDER, self.raspGPIO.LOW)\n",
    "        self.raspGPIO.output(self.RDDR, self.raspGPIO.LOW)\n",
    "        self.raspGPIO.output(self.RTEF, self.raspGPIO.LOW)\n",
    "        self.raspGPIO.output(self.RTDF, self.raspGPIO.LOW)\n",
    "        self.raspGPIO.output(self.RTER, self.raspGPIO.LOW)\n",
    "        self.raspGPIO.output(self.RTDR, self.raspGPIO.LOW)\n",
    "\n",
    "    def forward(self):\n",
    "        self.raspGPIO.output(self.RDEF, self.raspGPIO.HIGH)\n",
    "        self.raspGPIO.output(self.RDDF, self.raspGPIO.HIGH)\n",
    "        self.raspGPIO.output(self.RTEF, self.raspGPIO.HIGH)\n",
    "        self.raspGPIO.output(self.RTDF, self.raspGPIO.HIGH)\n",
    "    \n",
    "\n",
    "    def backward(self):\n",
    "        self.raspGPIO.output(self.RDER, self.raspGPIO.HIGH)\n",
    "        self.raspGPIO.output(self.RDDR, self.raspGPIO.HIGH)\n",
    "        self.raspGPIO.output(self.RTER, self.raspGPIO.HIGH)\n",
    "        self.raspGPIO.output(self.RTDR, self.raspGPIO.HIGH)\n",
    "        \n",
    "    def left_forward(self):\n",
    "        self.raspGPIO.output(self.RDEF, self.raspGPIO.LOW)\n",
    "        self.raspGPIO.output(self.RDDF, self.raspGPIO.HIGH)\n",
    "        self.raspGPIO.output(self.RTDF, self.raspGPIO.HIGH)\n",
    "        self.raspGPIO.output(self.RTEF, self.raspGPIO.HIGH)\n",
    "\n",
    "\n",
    "\n",
    "    def right_forward(self):\n",
    "        self.raspGPIO.output(self.RDEF, self.raspGPIO.HIGH)\n",
    "        self.raspGPIO.output(self.RDDF, self.raspGPIO.LOW)\n",
    "        self.raspGPIO.output(self.RTDF, self.raspGPIO.HIGH)\n",
    "        self.raspGPIO.output(self.RTEF, self.raspGPIO.HIGH)\n",
    "\n",
    "    def left_backward(self):\n",
    "        self.raspGPIO.output(self.RDER, self.raspGPIO.LOW)\n",
    "        self.raspGPIO.output(self.RDDR, self.raspGPIO.HIGH)\n",
    "        self.raspGPIO.output(self.RTDR, self.raspGPIO.HIGH)\n",
    "        self.raspGPIO.output(self.RTER, self.raspGPIO.HIGH)\n",
    "\n",
    "    def right_backward(self):\n",
    "        self.raspGPIO.output(self.RDER, self.raspGPIO.HIGH)\n",
    "        self.raspGPIO.output(self.RDDR, self.raspGPIO.LOW)\n",
    "        self.raspGPIO.output(self.RTDR, self.raspGPIO.HIGH)\n",
    "        self.raspGPIO.output(self.RTER, self.raspGPIO.HIGH)\n",
    "    \n",
    "    def movimentacarro(self, movimento):\n",
    "        if movimento==0:\n",
    "            self.forward()\n",
    "            time.sleep(1)\n",
    "        elif movimento==1:\n",
    "             self.backward()\n",
    "             time.sleep(1)\n",
    "        elif movimento==2:\n",
    "             self.left_forward()\n",
    "             time.sleep(2)\n",
    "        elif movimento==3:\n",
    "             self.right_forward()\n",
    "             time.sleep(2)\n",
    "        elif movimento==4:\n",
    "             self.left_backward()\n",
    "             time.sleep(1)\n",
    "        elif movimento==5:\n",
    "             self.right_backward()\n",
    "             time.sleep(1)\n",
    "\n",
    "        self.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gsm_GPS:\n",
    "    def __init__(self):\n",
    "        self.port=serial.Serial(\"/dev/tty8\",baudrate=11500, timeout=5  )\n",
    "        self.port.flush()\n",
    "        self.habilita_gsm_gps()\n",
    "    \n",
    "    def habilita_gsm_gps(self):\n",
    "        self.port.write('AT\\r'.encode())\n",
    "        self.response()\n",
    "        self.port.write('AT+CSQ\\r'.encode())\n",
    "        self.response()\n",
    "        port.write('AT+CPIN?\\r')\n",
    "        response()\n",
    "    \n",
    "    def response(self):\n",
    "        responseCount = 0\n",
    "        while True:\n",
    "            resp = self.port.readline().decode()\n",
    "            print(resp)\n",
    "            print(responseCount)\n",
    "            if \"OK\" in resp:\n",
    "                break\n",
    "            if \"ERROR\" in resp:\n",
    "                break\n",
    "            if responseCount > 30:\n",
    "                break\n",
    "            else:\n",
    "                responseCount +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#gsmgps = Gsm_GPS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "#movimentação do servo motor\n",
    "\n",
    "class StepMotor:\n",
    "    def __init__(self, raspGPIO, radar):\n",
    "        self.controlPinEsquerda = [26,19,13,6]\n",
    "        self.controlPinDireita = [6,13,19,26]\n",
    "        self.posicoesMedicaRadar = [0,50, 100, 150, 200, 250, 299]\n",
    "        self.medicaoRadar=[]\n",
    "        self.raspGPIO = raspGPIO\n",
    "        self.posicao_final = 0 #Esquerda\n",
    "        \n",
    "        self.seq1 = [[1,0,0,0], \n",
    "               [1,1,0,0],\n",
    "               [0,1,0,0],\n",
    "               [0,1,1,0],\n",
    "               [0,0,1,0],\n",
    "               [0,0,1,1],\n",
    "               [0,0,0,1],\n",
    "               [1,0,0,1]]\n",
    "        self.seq2 = [\n",
    "                [1,0,0,1],\n",
    "                [1,0,0,0], \n",
    "                [1,1,0,0],\n",
    "                [0,1,0,0],\n",
    "                [0,1,1,0],\n",
    "                [0,0,1,0],\n",
    "                [0,0,1,1],\n",
    "                [0,0,0,1]]\n",
    "        self.parar_motor()\n",
    "\n",
    "    def parar_motor(self):\n",
    "        for pin in self.controlPinEsquerda:\n",
    "            raspGPIO.setup(pin, raspGPIO.OUT)\n",
    "            raspGPIO.output(pin, False)    \n",
    "    \n",
    "    def moverMotorEsquerda(self):\n",
    "        self.medicaoRadar=[]\n",
    "        for i in range(0, 300,1):\n",
    "            if i in self.posicoesMedicaRadar:\n",
    "                self.medicaoRadar.append(radar.get_radar_posicao())\n",
    "                        \n",
    "            for halfstep in range(8):\n",
    "                for pin in range(4):\n",
    "                    self.raspGPIO.output(self.controlPinEsquerda[pin], self.seq1[halfstep][pin])\n",
    "                    time.sleep(0.001)\n",
    "\n",
    "    def moverMotorDireita(self):\n",
    "        self.medicaoRadar=[]\n",
    "        for i in range(0,300,1):\n",
    "            if i in self.posicoesMedicaRadar:\n",
    "                self.medicaoRadar.append(radar.get_radar_posicao())\n",
    "            for halfstep in range(8):\n",
    "                for pin in range(4):\n",
    "                    self.raspGPIO.output(self.controlPinDireita[pin], self.seq1[halfstep][pin])\n",
    "                    time.sleep(0.001)\n",
    "    \n",
    "\n",
    "    def moverMotorDireita_ajuste(self, valor):\n",
    "        for i in range(0,valor,1):\n",
    "            for halfstep in range(8):\n",
    "                for pin in range(4):\n",
    "                    self.raspGPIO.output(self.controlPinDireita[pin], self.seq1[halfstep][pin])\n",
    "                    time.sleep(0.001)\n",
    "       \n",
    "    def moverMotorEsquerda_ajuste(self, valor):\n",
    "        for i in range(0, valor, 1):\n",
    "            for halfstep in range(8):\n",
    "                for pin in range(4):\n",
    "                    self.raspGPIO.output(self.controlPinEsquerda[pin], self.seq1[halfstep][pin])\n",
    "                    time.sleep(0.001)\n",
    "        \n",
    "    def get_posicoes_radar(self):\n",
    "        if self.posicao_final == 0:\n",
    "            self.moverMotorEsquerda()\n",
    "            self.posicao_final = 1\n",
    "        else:\n",
    "            self.moverMotorDireita()\n",
    "            self.posicao_final = 0\n",
    "            self.medicaoRadar = self.medicaoRadar[::-1] \n",
    "            \n",
    "        return self.medicaoRadar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vl53_Radar:\n",
    "    def __init__(self):\n",
    "        self.i2c = busio.I2C(board.SCL, board.SDA)\n",
    "        self.vl53 = adafruit_vl53l0x.VL53L0X(self.i2c)\n",
    "        self.yarr = []\n",
    "        self.count = 0\n",
    "\n",
    "    \n",
    "    def get_radar(self):\n",
    "        return self.vl53\n",
    "    \n",
    "    def get_radar_posicao(self):\n",
    "        return int(self.vl53.range / 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir(base):\n",
    "    ''''\n",
    "    Cria diretórios para salvar as matrizes \n",
    "    '''\n",
    "    path = os.path.join('save', base)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    return path\n",
    "\n",
    "def salvaMatrizes(matrizNome, matrizValor):\n",
    "    ''''\n",
    "    Cria diretórios para salvar as matrizes \n",
    "    Salva as matrizes com o nome e o valor passado como parametro\n",
    "    '''\n",
    "    today=datetime.today().strftime('%Y-%m-%d')\n",
    "    mkdir(today)\n",
    "    daytime=datetime.today().strftime('%H:%M:%S')\n",
    "    np.savez(('save/%s/%s_%s.npz' % (today, matrizNome, daytime)), matrizValor)\n",
    "\n",
    "def carregaMatriz(pasta, matrizNome):\n",
    "    ''''\n",
    "    Carrega as matrizes salvas no diretório\n",
    "    '''\n",
    "    path = os.path.join('save', pasta, matrizNome)\n",
    "    load = np.load(path)\n",
    "    return load['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CarEnv:\n",
    "    def __init__(self, motorCarro, radarPosicoes, StepMotor):\n",
    "        self.actions_space = ['forward', 'backward', 'leftforward', 'rightforward', 'leftbackward', 'rightbackward']\n",
    "        self.observation_space = np.array(np.zeros([len(radarPosicoes), len(self.actions_space)]))\n",
    "        self.state = radarPosicoes\n",
    "        self.done = False\n",
    "        self.input_size = len(radarPosicoes)\n",
    "        self.output_size = len(self.actions_space)\n",
    "        self.motorCar =  motorCarro\n",
    "        self.finishCount = 0\n",
    "    \n",
    "    def finish(self, state):\n",
    "        '''\n",
    "        Quando o carro se movimentar 3 vezes para a frente sem parar é o objetivo dele\n",
    "        '''\n",
    "        print(\"STATE FINISH:\", state)\n",
    "        if(state[2]>100 and state[3]>100 and state[4]>100):\n",
    "            self.finishCount+=1\n",
    "        else:\n",
    "            self.finishCount=0\n",
    "        \n",
    "        print(\"Count FINISH:\", self.finishCount)\n",
    "\n",
    "        self.finishStatus = False\n",
    "        if self.finishCount>=3:\n",
    "            self.finishStatus = True\n",
    "        return self.finishStatus\n",
    "\n",
    "    def step(self, action):\n",
    "        self.take_action(action)\n",
    "        self.state = self.getState()\n",
    "        self.done = self.finish(self.state)\n",
    "        stepP = copy.deepcopy(self.getReward()), self.state, self.done\n",
    "        return stepP\n",
    "\n",
    "    def getState(self):\n",
    "        try:\n",
    "            self.state = stepMotor.get_posicoes_radar()\n",
    "        finally:\n",
    "           stepMotor.parar_motor()\n",
    "        return self.state\n",
    "        \n",
    "    def take_action(self, action):\n",
    "        movPosition = np.where(action == np.max(action)) \n",
    "        l = list(action).index(np.max(action))\n",
    "        print('action', l, np.max(action), action)\n",
    "        self.motorCar.movimentacarro(l)\n",
    "\n",
    "    def getReward(self):\n",
    "        f = lambda x: 10 if  x>100 else -10\n",
    "        return f(max(self.state[2:5]))\n"
   ]
  },
  {
   "source": [
    "### Inicialização dos Hiperparametros\n",
    "Neste parte implementaremos a seguinte parte do código\n",
    "\n",
    "<img src=\"imagens/ars_part1.png\" width=600 heigth=400>\n",
    "\n",
    "Inicialmente vamos usar steps_size=10 e epsodes=10 apenas para fins de comparação de resultados, ja que o ambiente é real de um carro autonomo\n",
    "\n",
    "self.directions = Total de matrizes de pertubações a serem contruidas ***number of directions sampled per iteration N***\n",
    "\n",
    "self.best_directions = Total de matrizes com recompensas melhores \n",
    "\n",
    "Nunca as matrizes de pertubação pode ser maior que as matrizes de recompensas, por isso o uso do assert\n",
    "\n",
    "***(number of top-performing directions to use b (b < N is allowed only for V1-t and V2-t)***\n",
    "\n",
    "self.noise = noise ν"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hiperparametros():\n",
    "    def __init__(self):\n",
    "        self.steps = 10\n",
    "        self.epsodes = 10\n",
    "        self.lr = 0.02\n",
    "        self.directions = 6\n",
    "        self.best_directions = 6\n",
    "        assert self.best_directions <= self.directions\n",
    "        self.noise = 0.03\n",
    "        self.seed = 1\n",
    "        self.env_nome = ''\n"
   ]
  },
  {
   "source": [
    "Os bons resultados desse algoritmo se dão fortemente por causa da normalização dos dados\n",
    "\n",
    "De acordo com documento, a normalização é necessário por:\n",
    "\n",
    "\"A normalização de estados usada por V2 é semelhante ao clareamento de dados usado em tarefas de regressão, e\n",
    "intuitivamente, garante que as políticas atribuam peso igual aos diferentes componentes dos estados. Para\n",
    "obter intuição de por que isso pode ajudar, suponha que uma coordenada de estado só tenha valores no intervalo\n",
    "90, 100 enquanto outro componente de estado assume valores na faixa -1, 1. Então, pequenas mudanças em\n",
    "o ganho de controle em relação à primeira coordenada de estado levaria a mudanças maiores nas ações\n",
    "então, o mesmo tamanho muda em relação ao segundo componente de estado. Portanto, o clareamento permite\n",
    "a exploração isotrópica de pesquisa aleatória para ter igual influência sobre os vários componentes de estado\"\n",
    "\n",
    "\n",
    "Nesse caso vamos normalizar os valores recebidos pelo radar que varia entre 1 e 899, que seriam equivalentes há 1cm e 89,9cm. \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizacao dos estados (Standardization)\n",
    "class Normalizacao():\n",
    "    def __init__(self, inputs):\n",
    "        '''\n",
    "        Inicializa todos os parametros utilizados durante a normalização\n",
    "        Keyword arguments:\n",
    "        inputs -- array dos valores recebidos do sensor de distancia, ex: [30, 6, 3, 4, 8, 91, 819]\n",
    "        '''\n",
    "        self.n = np.zeros(inputs) #agregador de estados descobertos desde o inicio\n",
    "        self.mean = np.zeros(inputs) #média de todos os valores de input\n",
    "        self.mean_diff = np.zeros(inputs) #usado para o calculo da variancia\n",
    "        self.var = np.zeros(inputs) #guarda os valores de variancia\n",
    "    \n",
    "    def observe(self, inputs):\n",
    "        '''\n",
    "        Realiza o calculo da variancia nos dados recebidos do sensor de movimento\n",
    "        Keyword arguments:\n",
    "        inputs -- array dos valores recebidos do sensor de distancia, ex: [30, 6, 3, 4, 8, 91, 819]\n",
    "        '''\n",
    "        self.n +=1. #indica em que ação é a atual\n",
    "        last_mean = self.mean.copy() #guarda o valor da ultima média realizada \n",
    "        self.mean += (inputs - self.mean) / self.n #atualizando a média baseada na quantidade de ações ja realizadas\n",
    "        self.mean_diff += (inputs - last_mean) * (inputs - self.mean) # pega a diferença atual e a média antiga\n",
    "        self.var = (self.mean_diff/self.n).clip(min = 1e-2) #realiza o calculo da variancia, e limita o valor minino em 0.01\n",
    "\n",
    "    def normalize(self, inputs):\n",
    "        ''''\n",
    "        Realiza o calculo da normalização(Padronização) dividindo realizando x- média(x) / desvio padrão de X\n",
    "        Assim deixando todos os valores dentro da escala -1 e 1\n",
    "        Essa forma é mais robusta contra outliers\n",
    "\n",
    "        Keyword arguments:\n",
    "        inputs -- array dos valores recebidos do sensor de distancia, ex: [30, 6, 3, 4, 8, 91, 819]\n",
    "\n",
    "        Return:\n",
    "        Normalized - Valores normalizados entre -1 e 1\n",
    "        '''\n",
    "        obs_mean = self.mean #Qual a média atual\n",
    "        obs_std = np.sqrt(self.var) #Calcula Desvio padrão\n",
    "        normalized = (inputs - obs_mean) / obs_std #(valor a ser nomalizado - média) / desvio padrão \n",
    "        return normalized"
   ]
  },
  {
   "source": [
    "Neste parte implementaremos as seguintes partes do código\n",
    "\n",
    "<img src=\"imagens/ars_part2.png\" width=600 heigth=400>\n",
    "\n",
    "<img src=\"imagens/ars_part3.png\" width=600 heigth=400>\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Politicas():\n",
    "    '''\n",
    "        A exploração acontece no espaço das politicas depois que todo um episódio e steps dele foram executados\n",
    "        Diferente de outras IAs que usam exploração por ambientes e ações executadas\n",
    "    '''\n",
    "\n",
    "    def __init__(self, input_size, output_size):\n",
    "        '''\n",
    "        Keyword arguments:\n",
    "        input_size -- Numeros de entradas\n",
    "        output_size -- Numeros de saídas\n",
    "\n",
    "        Return:\n",
    "        '''\n",
    "        self.theta = np.zeros((output_size, input_size)) #Cria uma matrix de pesos inicializado em zeros\n",
    "        #Estamos seguindo o método do artivo pelo lado esquerdo, por isso usamos (output_size, input_size)\n",
    "    \n",
    "    def evaluate(self, input, delta=None, direction=None):\n",
    "        ''''\n",
    "        De acordo com a direção passada em direction é atualizada a matriz de pesos\n",
    "\n",
    "        Keyword arguments:\n",
    "        inputs -- array dos valores recebidos do sensor de distancia, ex: [30, 6, 3, 4, 8, 91, 819]\n",
    "        delta -- Matrix de pertubação dos numeros \n",
    "        direction -- Indica a direção do calculo para positivo ou negativo\n",
    "        '''\n",
    "\n",
    "        if direction is None:\n",
    "            return self.theta.dot(input) #retorna a matriz de pesos que multiplica com as entradas, sem pertubações\n",
    "        elif direction == 'positive':\n",
    "            return (self.theta + hp.noise * delta).dot(input) #retorna a matriz de pesos que multiplica com as entradas, com ruido de exploração mais pertubações positivas\n",
    "        else: \n",
    "            return (self.theta - hp.noise * delta).dot(input)#retorna a matriz de pesos que multiplica com as entradas, com ruido de exploração mais pertubações negativas\n",
    "    \n",
    "    def samples_deltas(self):\n",
    "        '''\n",
    "        Gerando matrix de pertubação, matrix com numeros aleatórios\n",
    "        '''\n",
    "        return [np.random.randn(*self.theta.shape) for _ in range(hp.directions)]\n",
    "        #retorna uma matrix com distribuição normal para o todas as matrizes de pertubaçao \n",
    "    \n",
    "    def update(self, rollouts, sigma_r):\n",
    "        '''\n",
    "        Item 7 do Algoritmo, fazendo a atualização dos pesos\n",
    "\n",
    "        Keyword arguments:\n",
    "        rollouts -- conjunto de recompensa positiva, conjunto recompensa negativa e a matrizx de numeros aleatórios\n",
    "        sigma_r -- Indica o desvio padrão da recompensa\n",
    "        '''\n",
    "        step = np.zeros(self.theta.shape) #Inicializa com as dimensões de pesos\n",
    "        for r_pos, r_neg, d in rollouts:\n",
    "            step += (r_pos - r_neg) * d #Somatoria das recompensas positivas e negativas e a multicao do delta\n",
    "        self.theta += hp.learning_rate / (hp.best_directions * sigma_r) * step #Atualizando a matriz  de pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore(env, normalizer, policy, direction = None, delta=None):\n",
    "    '''\n",
    "         Faz a exploração do ambiente enquanto não finalizar e não terminar as execuções do epsodio\n",
    "      \n",
    "    '''\n",
    "    state = env.getState() #ler o ambiente radar\n",
    "    done = False #inicia em False o objetivo\n",
    "    num_plays = 0. #contador de rodadas no episódio\n",
    "    sum_rewards = 0 #soma das recompensas\n",
    "    while not done and num_plays < hp.epsodes:\n",
    "       print('Execucao: ', num_plays, ', Episodio: ', hp.epsodes, ' Finalizado: ', done)\n",
    "       normalizer.observe(state) #Atualiza o calculo da variancia nos dados recebidos do sensor de movimento\n",
    "       state = normalizer.normalize(state) #Realiza o calculo da normalização(Padronização) deixando todos os estados entre -1 e 1\n",
    "       action = policy.evaluate(state, delta, direction) #atualizada a matriz de pesos de acordo com a direçao selecionada e retirna \n",
    "       reward, state, done = env.step(action) #Executa a ação selecionada e retirna a nova leitura do ambiente e se foi finalizado \n",
    "       print(\"Recompensa: \", reward)\n",
    "       reward = max(min(reward, 1), -1)#evita outlier nas recompensas\n",
    "       sum_rewards += reward #Soma das recompensas\n",
    "       num_plays +=1 #atualida o numero da rodada\n",
    "    return sum_rewards\n"
   ]
  },
  {
   "source": [
    "Neste parte implementaremos as seguintes partes do código\n",
    "\n",
    "<img src=\"imagens/ars_part4.png\" width=600 heigth=400>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Treinando\n",
    "def train(env, policy, normalizer, hp):\n",
    "    ''''\n",
    "      Realiza o treinamento da rede\n",
    "   \n",
    "    '''\n",
    "    loadMatrixPositiveFilename = None\n",
    "    LoadMatrixFolder = None\n",
    "    loadMatrixNegativeFilename = None\n",
    "    DeltaFilename = None\n",
    "    for step in range(hp.steps):\n",
    "        \n",
    "        if loadMatrixPositiveFilename and loadMatrixNegativeFilename and LoadMatrixFolder and DeltaFilename:\n",
    "            deltas = carregaMatriz(LoadMatrixFolder, loadMatrixPositiveFilename) #Inicializacao das pertubacoes (deltas) e as recompensas negativas e positivas)\n",
    "            #positive_rewards = \n",
    "            negative_rewards = carregaMatriz(LoadMatrixFolder, DeltaFilename)\n",
    "        else:\n",
    "            deltas = policy.samples_deltas() #Inicializacao das pertubacoes (deltas) e as recompensas negativas e positivas)\n",
    "            positive_rewards = [0] * hp.directions #inicializando a matriz de recompensas positivas\n",
    "            negative_rewards = [0] * hp.directions #inicializando a matriz de recompensas negativas\n",
    "\n",
    "        #obtendo as recompensas na direcao positiva\n",
    "        for k in range(hp.directions):\n",
    "            positive_rewards[k] = explore(env, normalizer, policy, direction='positive', delta=deltas[k])\n",
    "            \n",
    "        #obtendo recompensa na direcao negativa\n",
    "        for k in range(hp.directions):\n",
    "            positive_rewards[k] = explore(env, normalizer, policy,  direction='negative', delta=deltas[k])\n",
    "        \n",
    "        #obtendo todas as recompensas positivas e negativas para computar o desvio dessas recompensas\n",
    "        all_reward = np.array(positive_rewards + negative_rewards)\n",
    "        sigma_r = all_rewards.std()\n",
    "\n",
    "        #ordenacao dos rollouts e selecao das melhores direcoes\n",
    "        scores = {k: max(r_pos, r_neg) for k, (r_pos, r_neg) in enumerate(zip(positive_rewards, negative_rewards))}\n",
    "        order = sorted(scores.keys(), key=lambda x: scores[x], reverse=True)[:hp.best_directions]\n",
    "        rollouts = [(positive_rewards[k], negative_rewards[k], deltas [k]) for k in order]\n",
    "\n",
    "        #atualizacao de politica\n",
    "        policy.update(rollouts, sigma_r)\n",
    "\n",
    "        #impressao da recompensa\n",
    "        reward_evaluation = explore(env, normalizer, policy)\n",
    "        print('Step', step, 'Reward:',reward_evaluation )\n",
    "\n",
    "    salvaMatrizes('positive_rewards', positive_rewards)\n",
    "    salvaMatrizes('negative_rewards', negative_rewards)\n",
    "    alvaMatrizes('deltas', deltas)\n",
    "\n",
    "    \n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "raspGPIO  = SetupGPIO().get_gpio()\n",
    "radar     = Vl53_Radar()\n",
    "stepMotor = StepMotor(raspGPIO, radar)\n",
    "motorCar = MotorCarro(raspGPIO)\n",
    "carEnv = CarEnv(motorCar, stepMotor.get_posicoes_radar(),stepMotor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Execucao:  0.0 , Episodio:  10  Finalizado:  False\naction 0 0.0 [0. 0. 0. 0. 0. 0.]\nRecompensa:  10\nExecucao:  1.0 , Episodio:  10  Finalizado:  False\naction 0 0.10974709017905396 [ 0.10974709 -0.07301745  0.08381035  0.01854194  0.00093107  0.08778463]\nRecompensa:  10\nExecucao:  2.0 , Episodio:  10  Finalizado:  False\naction 4 0.003023498059221339 [-0.09707547 -0.05484158 -0.03575612 -0.03677758  0.0030235  -0.02669773]\nRecompensa:  10\nExecucao:  3.0 , Episodio:  10  Finalizado:  False\naction 5 0.05512744928456858 [-0.10314292 -0.06644181  0.03960878 -0.03785251 -0.05893177  0.05512745]\nRecompensa:  10\nExecucao:  4.0 , Episodio:  10  Finalizado:  False\naction 1 0.07890132557251969 [-0.00900117  0.07890133  0.01415069  0.03451897  0.06220336  0.04410733]\nRecompensa:  10\nExecucao:  5.0 , Episodio:  10  Finalizado:  False\naction 4 0.10026044205389141 [0.03371261 0.08663423 0.01415866 0.00204447 0.10026044 0.01838037]\nRecompensa:  10\nExecucao:  6.0 , Episodio:  10  Finalizado:  False\naction 1 0.059027132812212675 [-0.02297306  0.05902713  0.01026332  0.03288791  0.0406875   0.03195753]\nRecompensa:  10\nExecucao:  7.0 , Episodio:  10  Finalizado:  False\naction 3 0.09821029608631873 [-0.03904257  0.08555244 -0.05449682  0.0982103   0.0930047  -0.02479727]\nRecompensa:  10\nExecucao:  8.0 , Episodio:  10  Finalizado:  False\naction 2 0.025351169482897905 [-0.02329773 -0.0943077   0.02535117 -0.09487296 -0.05282308  0.01826208]\nRecompensa:  10\nExecucao:  9.0 , Episodio:  10  Finalizado:  False\naction 5 -0.0053762781946534235 [-0.03909271 -0.11938496 -0.01285707 -0.10061808 -0.13488574 -0.00537628]\nRecompensa:  10\nExecucao:  0.0 , Episodio:  10  Finalizado:  False\naction 3 0.07892889274165935 [ 0.05507766  0.00072169 -0.00744553  0.07892889 -0.04168357 -0.03316522]\nRecompensa:  10\nExecucao:  0.0 , Episodio:  10  Finalizado:  False\naction 0 0.13924283865796738 [ 0.13924284 -0.08419613 -0.16197506 -0.12555139  0.06094389 -0.28306127]\nRecompensa:  10\nExecucao:  1.0 , Episodio:  10  Finalizado:  False\naction 0 0.1808345463344463 [ 0.18083455 -0.07226632 -0.18330712 -0.011864    0.1184099  -0.12257898]\nRecompensa:  10\nExecucao:  2.0 , Episodio:  10  Finalizado:  False\naction 4 0.18951468563004453 [ 0.05835875 -0.04140267 -0.18006333  0.07500617  0.18951469 -0.01282773]\nRecompensa:  -10\nExecucao:  3.0 , Episodio:  10  Finalizado:  False\naction 0 0.15876564988926917 [ 0.15876565 -0.02304109 -0.08641427  0.02765805  0.03159275 -0.09686163]\nRecompensa:  10\nExecucao:  4.0 , Episodio:  10  Finalizado:  False\naction 3 0.08556555102289687 [ 0.00418159  0.02854572  0.0346594   0.08556555 -0.01990602  0.03573977]\nRecompensa:  10\nExecucao:  5.0 , Episodio:  10  Finalizado:  False\naction 0 0.15125492438526683 [ 0.15125492 -0.01411904 -0.05030911  0.02627838  0.02244756  0.03276698]\nRecompensa:  -10\nExecucao:  6.0 , Episodio:  10  Finalizado:  False\naction 0 0.10876261381949409 [ 0.10876261 -0.00614817 -0.07730917 -0.0289944   0.02714067 -0.11575394]\nRecompensa:  10\nExecucao:  7.0 , Episodio:  10  Finalizado:  False\naction 0 0.10680056252617152 [ 0.10680056 -0.00027363 -0.04623832 -0.02464421  0.02244598  0.01283302]\nRecompensa:  10\nExecucao:  8.0 , Episodio:  10  Finalizado:  False\naction 0 0.09902839965957211 [ 0.0990284   0.00025408 -0.04258631 -0.02294651  0.02070744  0.01262943]\nRecompensa:  -10\nExecucao:  9.0 , Episodio:  10  Finalizado:  False\naction 0 0.08553737117197414 [ 0.08553737 -0.003942   -0.06351285 -0.02353443  0.02151357 -0.10421339]\nRecompensa:  -10\nExecucao:  0.0 , Episodio:  10  Finalizado:  False\naction 3 0.14251292503678958 [-0.10086398  0.02573703 -0.05022211  0.14251293 -0.05867989  0.05518262]\nRecompensa:  -10\nExecucao:  1.0 , Episodio:  10  Finalizado:  False\naction 3 0.13976035723491986 [-0.09594302  0.02631351 -0.04898476  0.13976036 -0.05578554  0.05299566]\nRecompensa:  10\nExecucao:  2.0 , Episodio:  10  Finalizado:  False\naction 5 0.03976471384023305 [-0.10134393 -0.04438015 -0.01365232 -0.01562462 -0.05205279  0.03976471]\nRecompensa:  10\nExecucao:  3.0 , Episodio:  10  Finalizado:  False\naction 1 0.046476226521819736 [ 0.03466808  0.04647623 -0.02945371  0.04276244 -0.02707616 -0.01218733]\nRecompensa:  10\nExecucao:  4.0 , Episodio:  10  Finalizado:  False\naction 0 0.07816131865650855 [ 0.07816132  0.07217878  0.01547912  0.02428843 -0.03338789  0.01686793]\nRecompensa:  -10\nExecucao:  5.0 , Episodio:  10  Finalizado:  False\naction 3 0.16154988705020934 [ 0.0159216   0.05504356 -0.05561012  0.16154989 -0.01974939 -0.01685068]\nRecompensa:  10\nExecucao:  6.0 , Episodio:  10  Finalizado:  False\naction 5 0.03805725130049135 [-0.10050717 -0.04779124 -0.0125344  -0.02197798 -0.04647121  0.03805725]\nRecompensa:  10\nExecucao:  7.0 , Episodio:  10  Finalizado:  False\naction 3 0.019893948587357116 [ 0.0086413  -0.00842967 -0.02127588  0.01989395 -0.01731487 -0.02593203]\nRecompensa:  -10\nExecucao:  8.0 , Episodio:  10  Finalizado:  False\naction 3 0.1660562805149497 [ 0.01783254  0.05955185 -0.05442989  0.16605628 -0.01767291 -0.0146401 ]\nRecompensa:  -10\nExecucao:  9.0 , Episodio:  10  Finalizado:  False\naction 3 0.1207712018880604 [-0.08541514  0.02208874 -0.04212276  0.1207712  -0.0446673   0.04790422]\nRecompensa:  -10\nExecucao:  0.0 , Episodio:  10  Finalizado:  False\naction 5 0.10782127133819755 [-0.09206069 -0.04886935 -0.02120302 -0.13275048 -0.05024883  0.10782127]\nRecompensa:  10\nExecucao:  1.0 , Episodio:  10  Finalizado:  False\naction 0 0.19571244038941527 [ 0.19571244 -0.07740188  0.04526148 -0.059517   -0.03116347  0.16253699]\nRecompensa:  10\nExecucao:  2.0 , Episodio:  10  Finalizado:  False\naction 3 0.07684142685093985 [ 0.06946116 -0.04312647 -0.03443663  0.07684143  0.04167814 -0.16646026]\nRecompensa:  10\nExecucao:  3.0 , Episodio:  10  Finalizado:  False\naction 3 0.08112151071884219 [ 0.06627953 -0.03430903 -0.02207169  0.08112151  0.04580854 -0.15874871]\nRecompensa:  -10\nExecucao:  4.0 , Episodio:  10  Finalizado:  False\naction 5 0.17863385193011916 [-0.08772299  0.03975218 -0.028864   -0.16926455 -0.09733923  0.17863385]\nRecompensa:  -10\nExecucao:  5.0 , Episodio:  10  Finalizado:  False\naction 5 0.05390712132258452 [-0.08646814  0.01568895 -0.0394179  -0.06624987 -0.05559234  0.05390712]\nRecompensa:  10\nExecucao:  6.0 , Episodio:  10  Finalizado:  False\naction 0 0.14636667019050012 [ 0.14636667 -0.08643603  0.04513531 -0.02747167  0.02453671  0.04803202]\nRecompensa:  -10\nExecucao:  7.0 , Episodio:  10  Finalizado:  False\naction 5 0.16818252737968936 [-0.08619201  0.03844394 -0.02729821 -0.16043975 -0.09218747  0.16818253]\nRecompensa:  10\nExecucao:  8.0 , Episodio:  10  Finalizado:  False\naction 0 0.05213636948497807 [ 0.05213637 -0.07436192  0.00617227  0.04126299 -0.03132076 -0.06648214]\nRecompensa:  -10\nExecucao:  9.0 , Episodio:  10  Finalizado:  False\naction 5 0.17098018849801616 [-0.08543417  0.03963    -0.02869355 -0.16194156 -0.09235594  0.17098019]\nRecompensa:  -10\nExecucao:  0.0 , Episodio:  10  Finalizado:  False\naction 0 0.08026659690744535 [ 0.0802666  -0.01854096  0.02685706 -0.05492124 -0.01727383 -0.03014952]\nRecompensa:  -10\nExecucao:  1.0 , Episodio:  10  Finalizado:  False\naction 4 0.07191361806556125 [ 0.02912583 -0.12207708  0.0389178   0.01561439  0.07191362 -0.01003871]\nRecompensa:  10\nExecucao:  2.0 , Episodio:  10  Finalizado:  False\naction 0 0.13155544994639676 [ 0.13155545 -0.03781121 -0.0862283   0.06947725 -0.15238419 -0.01639712]\nRecompensa:  -10\nExecucao:  3.0 , Episodio:  10  Finalizado:  False\naction 3 0.07180233386708104 [-0.02687695 -0.15450561 -0.06019898  0.07180233  0.02083012  0.04395824]\nRecompensa:  -10\nExecucao:  4.0 , Episodio:  10  Finalizado:  False\naction 4 0.1256242952679105 [-0.00512275 -0.12669729  0.0162084   0.05656923  0.1256243   0.00039294]\nRecompensa:  -10\nExecucao:  5.0 , Episodio:  10  Finalizado:  False\naction 3 0.07057940039717253 [-0.02395882 -0.14702727 -0.06061683  0.0705794   0.01604985  0.04323938]\nRecompensa:  -10\nExecucao:  6.0 , Episodio:  10  Finalizado:  False\naction 3 0.06563541465492159 [-0.02357021 -0.14811494 -0.05856954  0.06563541  0.01768913  0.04091966]\nRecompensa:  -10\nExecucao:  7.0 , Episodio:  10  Finalizado:  False\naction 4 0.1204203510122599 [-0.00247653 -0.11809571  0.01799553  0.05190408  0.12042035 -0.00156595]\nRecompensa:  -10\nExecucao:  8.0 , Episodio:  10  Finalizado:  False\naction 3 0.06330932015603692 [-0.02221999 -0.14232079 -0.05714824  0.06330932  0.01441269  0.03967371]\nRecompensa:  -10\nExecucao:  9.0 , Episodio:  10  Finalizado:  False\naction 3 0.06251063496628483 [-0.02042363 -0.13992911 -0.05648377  0.06251063  0.013939    0.03829925]\nRecompensa:  -10\nExecucao:  0.0 , Episodio:  10  Finalizado:  False\naction 5 0.0010326167123532112 [-0.09149012 -0.01384258 -0.09974943 -0.05661594 -0.06853798  0.00103262]\nRecompensa:  -10\nExecucao:  1.0 , Episodio:  10  Finalizado:  False\naction 3 0.014849709817511388 [ 0.00857128 -0.05985276 -0.0286763   0.01484971 -0.08398052  0.0009858 ]\nRecompensa:  -10\nExecucao:  2.0 , Episodio:  10  Finalizado:  False\naction 5 0.0017976310697820318 [-0.09058801 -0.01243509 -0.09748955 -0.0550584  -0.06601665  0.00179763]\nRecompensa:  -10\nExecucao:  3.0 , Episodio:  10  Finalizado:  False\naction 3 0.017089916708585458 [ 0.01061413 -0.05894871 -0.02605907  0.01708992 -0.08161374  0.00136102]\nRecompensa:  -10\nExecucao:  4.0 , Episodio:  10  Finalizado:  False\naction 3 0.017378027310226387 [ 0.01095314 -0.05832929 -0.02547372  0.01737803 -0.08033033  0.00139254]\nRecompensa:  -10\nExecucao:  5.0 , Episodio:  10  Finalizado:  False\naction 5 0.0025689153218472274 [-0.09075256 -0.01012378 -0.09573181 -0.05343739 -0.06260774  0.00256892]\nRecompensa:  -10\nExecucao:  6.0 , Episodio:  10  Finalizado:  False\naction 3 0.01889596665814455 [ 0.01088746 -0.05660757 -0.02229219  0.01889597 -0.0768543   0.00318895]\nRecompensa:  10\nExecucao:  7.0 , Episodio:  10  Finalizado:  False\naction 0 0.1483713234825903 [ 0.14837132  0.03711675  0.02882415 -0.1663517   0.06753848 -0.07140777]\nRecompensa:  10\nExecucao:  8.0 , Episodio:  10  Finalizado:  False\naction 1 0.13201788165656533 [ 0.06344435  0.13201788 -0.02143268 -0.09715633  0.04630485  0.04038912]\nRecompensa:  10\nExecucao:  9.0 , Episodio:  10  Finalizado:  False\naction 4 0.10548518769160056 [ 0.03930265  0.05664466  0.08968233 -0.10254349  0.10548519 -0.02099161]\nRecompensa:  10\nExecucao:  0.0 , Episodio:  10  Finalizado:  False\naction 1 0.03394700854832324 [ 0.00679661  0.03394701 -0.11234116 -0.02673315 -0.01365313 -0.05770579]\nRecompensa:  10\nExecucao:  0.0 , Episodio:  10  Finalizado:  False\naction 1 0.012474893414075993 [-0.01724193  0.01247489 -0.02344421 -0.03221252 -0.00471357 -0.19360889]\nRecompensa:  10\nExecucao:  0.0 , Episodio:  10  Finalizado:  False\naction 5 0.03573662927302957 [-0.10394015 -0.05473755 -0.04763861 -0.00977962 -0.03489265  0.03573663]\nRecompensa:  10\nExecucao:  1.0 , Episodio:  10  Finalizado:  False\naction 0 0.10751571348185135 [ 0.10751571  0.00752747 -0.01339413 -0.11278881  0.00948303  0.00104117]\nRecompensa:  10\nExecucao:  2.0 , Episodio:  10  Finalizado:  False\naction 3 0.12501423159150138 [-0.09356369  0.0093719  -0.07518438  0.12501423 -0.03519991  0.04471069]\nRecompensa:  10\nExecucao:  3.0 , Episodio:  10  Finalizado:  False\naction 3 0.15836465724152773 [-0.0044502   0.01870015 -0.04897784  0.15836466  0.03839635 -0.07042686]\nRecompensa:  10\nExecucao:  4.0 , Episodio:  10  Finalizado:  False\naction 0 0.11389934049832112 [ 0.11389934 -0.02207834 -0.07083847 -0.14350566  0.07401878 -0.0735872 ]\nRecompensa:  10\nExecucao:  5.0 , Episodio:  10  Finalizado:  False\naction 5 0.07046783464009404 [-0.06737007 -0.02348901  0.00226276 -0.01589058 -0.05141664  0.07046783]\nRecompensa:  -10\nExecucao:  6.0 , Episodio:  10  Finalizado:  False\naction 2 0.056826132512907704 [-0.13066463 -0.05692294  0.05682613  0.01814134 -0.05991138  0.05566416]\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-176-d34d8edfb271>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnormalizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNormalizacao\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcarEnv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m    \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcarEnv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpolice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnormalizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m    \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-174-42ce36f2b8b7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(env, policy, normalizer, hp)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m#obtendo recompensa na direcao negativa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirections\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mpositive_rewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexplore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'negative'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeltas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m#obtendo todas as recompensas positivas e negativas para computar o desvio dessas recompensas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-173-5014cfc8935a>\u001b[0m in \u001b[0;36mexplore\u001b[0;34m(env, normalizer, policy, direction, delta)\u001b[0m\n\u001b[1;32m     13\u001b[0m        \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Realiza o calculo da normalização(Padronização) deixando todos os estados entre -1 e 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m        \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirection\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#atualizada a matriz de pesos de acordo com a direçao selecionada e retirna\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m        \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Executa a ação selecionada e retirna a nova leitura do ambiente e se foi finalizado\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m        \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Recompensa: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m        \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#evita outlier nas recompensas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-169-225d2e260011>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-169-225d2e260011>\u001b[0m in \u001b[0;36mtake_action\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'action'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmotorCar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmovimentacarro\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgetReward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-163-803aed340a93>\u001b[0m in \u001b[0;36mmovimentacarro\u001b[0;34m(self, movimento)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmovimento\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m              \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m              \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmovimento\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m              \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    " hp = Hiperparametros()\n",
    " np.random.seed(hp.seed)\n",
    " police = Politicas(carEnv.input_size, carEnv.output_size)\n",
    " normalizer = Normalizacao(carEnv.input_size)\n",
    " try:\n",
    "    train(carEnv,police,normalizer,hp)\n",
    "    pass\n",
    " finally:\n",
    "     stepMotor.parar_motor()\n",
    " #Fim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "stepMotor.moverMotorEsquerda_ajuste(300)\n",
    "##stepMotor.get_posicoes_radar()\n",
    "stepMotor.parar_motor()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "motorCar.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "motorCar.movimentacarro(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "tcc",
   "display_name": "tcc",
   "metadata": {
    "interpreter": {
     "hash": "85595fc6e4c66cea5298aa77223b4d67754825b590a0f7ffb3f639c26cc9de1a"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}